












































<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />

    
    <meta name="robots" content="noindex">
    

    <title>Ceph Docs</title>

    <link rel="canonical" href="https://rook.io/docs/rook/v0.6/common-problems.html">

    <link rel="icon" href="/favicon.ico" />
<link rel="icon" type="image/png" href="/images/favicon_16x16.png" sizes="16x16" />
<link rel="icon" type="image/png" href="/images/favicon_32x32.png" sizes="32x32" />
<link rel="icon" type="image/png" href="/images/favicon_48x48.png" sizes="48x48" />
<link rel="icon" type="image/png" href="/images/favicon_192x192.png" sizes="192x192" />


    <link href="//fonts.googleapis.com/css?family=Montserrat:500|Open+Sans:300,400,600" rel="stylesheet">
    
    <link rel="stylesheet" href="/css/main.css">
    
      <link rel="stylesheet" href="/css/docs.css" />
    
  </head>
  <body>
    <nav id="navigation" aria-label="Navigation">
  <div>
    <div class="logo">
      <a href="/"><img src="/images/rook-logo.svg"/></a>
    </div>
    <div
      class="hamburger-controls"
      onclick="if (document.body.classList.contains('menu-open')) { document.body.classList.remove('menu-open') } else { document.body.classList.add('menu-open') }; return false;">
      <span></span> <span></span> <span></span>
    </div>
    <ul class="links">
      <li class="dropdown">
        <a role="button" href="javascript:void(0)">Documentation</a>
        <div class="dropdown-content">
          <a href="/docs/rook/v1.9/">Ceph</a>
          <a href="/docs/cassandra/v1.7/">Cassandra</a>
          <a href="/docs/nfs/v1.7/">NFS</a>
        </div>
      <li class="dropdown">
        <a role="button" href="javascript:void(0)">Community</a>
        <div class="dropdown-content">
          <a href="//github.com/rook/rook">GitHub</a>
          <a href="//slack.rook.io/">Slack</a>
          <a href="//groups.google.com/forum/#!forum/rook-dev">Forum</a>
          <a href="//twitter.com/rook_io">Twitter</a>
        </div>
      </li>
      <li><a href="//blog.rook.io/">Blog</a></li>
      <li><a class="button small" href="/docs/rook/v1.9/quickstart.html">Get Started</a></li>
    </ul>
  </div>
</nav>

    <main id="content" aria-label="Content"><div>



















<section class="docs-header">
  <h1>Ceph</h1>
  <div class="versions">
    <a role="button" href="javascript:void(0)">Rook Ceph v0.6</a>
    <div class="versions-dropdown-content">
      
        <a href="/docs/rook/v1.9/common-problems.html">Rook Ceph v1.9</a>
      
        <a href="/docs/rook/v1.8/common-problems.html">Rook Ceph v1.8</a>
      
        <a href="/docs/rook/v1.7/common-problems.html">Rook Ceph v1.7</a>
      
        <a href="/docs/rook/v1.6/common-problems.html">Rook Ceph v1.6</a>
      
        <a href="/docs/rook/v1.5/common-problems.html">Rook Ceph v1.5</a>
      
        <a href="/docs/rook/v1.4/common-problems.html">Rook Ceph v1.4</a>
      
        <a href="/docs/rook/v1.3/common-problems.html">Rook Ceph v1.3</a>
      
        <a href="/docs/rook/v1.2/common-problems.html">Rook Ceph v1.2</a>
      
        <a href="/docs/rook/v1.1/common-problems.html">Rook Ceph v1.1</a>
      
        <a href="/docs/rook/v1.0/common-problems.html">Rook Ceph v1.0</a>
      
        <a href="/docs/rook/v0.9/common-problems.html">Rook Ceph v0.9</a>
      
        <a href="/docs/rook/v0.8/common-problems.html">Rook Ceph v0.8</a>
      
        <a href="/docs/rook/v0.7/common-problems.html">Rook Ceph v0.7</a>
      
        <a href="/docs/rook/v0.6/common-problems.html" class="active">Rook Ceph v0.6</a>
      
        <a href="/docs/rook/v0.5/common-problems.html">Rook Ceph v0.5</a>
      
        <a href="/docs/rook/latest/common-problems.html">Rook Ceph latest</a>
      
    </div>
    <img src="/images/arrow.svg" />
  </div>
</section>
<div class="page">
  <div class="docs-menu">
      <ul id="docs-ul"></ul>
  </div>
  <div class="docs-content">
    <div class="docs-actions">
      <a id="edit" href="https://github.com/rook/rook/blob/master/Documentation/common-problems.md">Edit on GitHub</a>
    </div>
    
      <div class="alert old">
        <p><b>PLEASE NOTE</b>: This document applies to v0.6 version and not to the latest <strong>stable</strong> release v1.9</p>
      </div>
    
    <div class="docs-text">
      <h1 id="common-problems">Common Problems</h1>

<p>Many of these problem cases are hard to summarize down to a short phrase that adequately describes the problem. Each problem will start with a bulleted list of symptoms. Keep in mind that all symptoms may not apply depending upon the configuration of the Rook. If the majority of the symptoms are seen there is a fair chance you are experiencing that problem.</p>

<p>If after trying the suggestions found on this page and the problem is not resolved, the Rook team is very happy to help you troubleshoot the issues in their Slack channel. Once you have <a href="https://slack.rook.io/">registered for the Rook Slack</a>, proceed to the General channel to ask for assistance.</p>

<h2 id="table-of-contents">Table of Contents</h2>
<ul>
  <li><a href="#pod-using-rook-storage-is-not-running">Pod using Rook storage is not running</a></li>
  <li><a href="#cluster-failing-to-service-requests">Cluster failing to service requests</a></li>
  <li><a href="#only-a-single-monitor-pod-starts-after-redeploy">Only a single monitor pod starts after redeploy</a></li>
</ul>

<h1 id="troubleshooting-techniques">Troubleshooting Techniques</h1>
<p>One of the first things that should be done is to start the <a href="/docs/rook/v0.6/toolbox.html">rook-tools pod</a> as described in the Toolbox section. Once the pod is up and running one can <code class="language-plaintext highlighter-rouge">kubectl exec</code> into the pod to execute Ceph commands to evaluate that current state of the cluster. Here is a list of commands that can help one get an understanding of the current state.</p>

<ul>
  <li>rookctl status</li>
  <li>ceph status</li>
  <li>ceph osd status</li>
  <li>ceph osd df</li>
  <li>ceph osd utilization</li>
  <li>ceph osd pool stats</li>
  <li>ceph osd tree</li>
  <li>ceph pg stat</li>
</ul>

<p>Of particular note, the first two status commands provide the overall cluster health. The normal state for cluster operations is HEALTH_OK, but will still function when the state is in a HEALTH_WARN state. If you are in a WARN state, then the cluster is in a condition that it may enter the HEALTH_ERROR state at which point <em>all</em> disk I/O operations are halted. If a HEALTH_WARN state is observed, then one should take action to prevent the cluster from halting when it enters the HEALTH_ERROR state.</p>

<p>There is literally a ton of Ceph sub-commands to look at and manipulate Ceph objects. Well beyond the scope of a few troubleshooting techniques and there are other sites and documentation sets that deal more with assisting one with troubleshooting a Ceph environment. In addition, there are other helpful hints and some best practices concerning a Ceph environment located in the <a href="/docs/rook/v0.6/advanced-configuration.html">Advanced Configuration section</a>. Of particular note there are scripts for collecting logs and gathering OSD information there.</p>

<h1 id="pod-using-rook-storage-is-not-running">Pod Using Rook Storage Is Not Running</h1>

<h2 id="symptoms">Symptoms</h2>
<ul>
  <li>The pod that is configured to use Rook storage is stuck in the <code class="language-plaintext highlighter-rouge">ContainerCreating</code> status</li>
  <li><code class="language-plaintext highlighter-rouge">kubectl describe pod</code> for the pod mentions one or more of the following:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">PersistentVolumeClaim is not bound</code></li>
      <li><code class="language-plaintext highlighter-rouge">timeout expired waiting for volumes to attach/mount</code></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">kubectl -n rook-system get pod</code> shows the rook-agent pods in a <code class="language-plaintext highlighter-rouge">CrashLoopBackOff</code> status</li>
</ul>

<h2 id="possible-solutions-summary">Possible Solutions Summary</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">rook-agent</code> pod is in a <code class="language-plaintext highlighter-rouge">CrashLoopBackOff</code> status because it cannot deploy its driver on a read-only filesystem: <a href="/docs/rook/v0.6/k8s-pre-reqs.html#flexvolume-configuration">Flexvolume configuration pre-reqs</a></li>
  <li>Persistent Volume and/or Claim are failing to be created and bound: <a href="#volume-creation">Volume Creation</a></li>
  <li><code class="language-plaintext highlighter-rouge">rook-agent</code> pod is failing to mount and format the volume: <a href="#volume-mounting">Rook Agent Mounting</a></li>
  <li>You are using Kubernetes 1.7.x or earlier and the Kubelet has not been restarted after <code class="language-plaintext highlighter-rouge">rook-agent</code> is in the <code class="language-plaintext highlighter-rouge">Running</code> status: <a href="#kubelet-restart">Restart Kubelet</a></li>
  <li>You are using Kubernetes 1.6.x and the attach-detach controller has not been disabled: <a href="/docs/rook/v0.6/kubernetes.html#disable-attacher-detacher-controller">Disable attach-detach controller</a></li>
</ul>

<h2 id="investigation-details">Investigation Details</h2>
<p>If you see some of the symptoms above, it’s because the requested Rook storage for your pod is not being created and mounted successfully.
In this walkthrough, we will be looking at the wordpress mysql example pod that is failing to start.
To first confirm there is an issue, you can run commands similar to the following and you should see similar output (note that some of it has been omitted for brevity):</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>kubectl get pod
<span class="go">NAME                              READY     STATUS              RESTARTS   AGE
wordpress-mysql-918363043-50pjr   0/1       ContainerCreating   0          1h

</span><span class="gp">&gt;</span><span class="w"> </span>kubectl describe pod wordpress-mysql-918363043-50pjr
<span class="c">...
</span><span class="go">Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath	Type		Reason			Message
  ---------	--------	-----	----			-------------	--------	------			-------
  1h		1h		3	default-scheduler			Warning		FailedScheduling	PersistentVolumeClaim is not bound: "mysql-pv-claim" (repeated 2 times)
  1h		35s		36	kubelet, 172.17.8.101			Warning		FailedMount		Unable to mount volumes for pod "wordpress-mysql-918363043-50pjr_default(08d14e75-bd99-11e7-bc4c-001c428b9fc8)": timeout expired waiting for volumes to attach/mount for pod "default"/"wordpress-mysql-918363043-50pjr". list of unattached/unmounted volumes=[mysql-persistent-storage]
  1h		35s		36	kubelet, 172.17.8.101			Warning		FailedSync		Error syncing pod
</span></code></pre></div></div>

<p>To troubleshoot this, let’s walk through the volume provisioning steps in order to confirm where the failure is happening.</p>

<h3 id="rook-agent-deployment">Rook Agent Deployment</h3>
<p>The <code class="language-plaintext highlighter-rouge">rook-agent</code> pods are responsible for mapping and mounting the volume from the cluster onto the node that your pod will be running on.
If the <code class="language-plaintext highlighter-rouge">rook-agent</code> pod is not running then it cannot perform this function.
Below is an example of the <code class="language-plaintext highlighter-rouge">rook-agent</code> pods failing to get to the <code class="language-plaintext highlighter-rouge">Running</code> status because they are in a <code class="language-plaintext highlighter-rouge">CrashLoopBackOff</code> status:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-system get pod
<span class="go">NAME                             READY     STATUS             RESTARTS   AGE
rook-agent-ct5pj                 0/1       CrashLoopBackOff   16         59m
rook-agent-zb6n9                 0/1       CrashLoopBackOff   16         59m
rook-operator-2203999069-pmhzn   1/1       Running            0          59m
</span></code></pre></div></div>
<p>If you see this occurring, you can get more details about why the <code class="language-plaintext highlighter-rouge">rook-agent</code> pods are continuing to crash with the following command and its sample output:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-system get pod <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>rook-agent <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{range .items[*]}{.metadata.name}{"\t"}{.status.containerStatuses[0].lastState.terminated.message}{"\n"}{end}'</span>
<span class="go">rook-agent-ct5pj	mkdir /usr/libexec/kubernetes: read-only file system
rook-agent-zb6n9	mkdir /usr/libexec/kubernetes: read-only file system
</span></code></pre></div></div>
<p>From the output above, we can see that the agents were not able to bind mount to <code class="language-plaintext highlighter-rouge">/usr/libexec/kubernetes</code> on the host they are scheduled to run on.
For some environments, this default path is read-only and therefore a better path must be provided to the agents.</p>

<p>First, clean up the agent deployment with:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl -n rook-system delete daemonset rook-agent
</span></code></pre></div></div>
<p>Once the <code class="language-plaintext highlighter-rouge">rook-agent</code> pods are gone, <strong>follow the instructions in the <a href="/docs/rook/v0.6/k8s-pre-reqs.html#flexvolume-configuration">Flexvolume configuration pre-reqs</a></strong> to ensure a good value for <code class="language-plaintext highlighter-rouge">--volume-plugin-dir</code> has been provided to the Kubelet.
After that has been configured, and the Kubelet has been restarted, start the agent pods up again by restarting <code class="language-plaintext highlighter-rouge">rook-operator</code>:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl -n rook-system delete pod -l app=rook-operator
</span></code></pre></div></div>

<h3 id="kubelet-restart">Kubelet Restart</h3>
<h4 id="kubernetes-17x-and-earlier-only"><strong>Kubernetes 1.7.x and earlier only</strong></h4>
<p>If the <code class="language-plaintext highlighter-rouge">rook-agent</code> pods are all in the <code class="language-plaintext highlighter-rouge">Running</code> state then another thing to confirm is that <strong>if you are running on Kubernetes 1.7.x or earlier</strong>, the Kubelet must be restarted after the <code class="language-plaintext highlighter-rouge">rook-agent</code> pods are running.</p>

<p>A symptom of this can be found in the Kubelet’s log/journal, with the following error saying <code class="language-plaintext highlighter-rouge">no volume plugin matched</code>:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Oct 30 22:23:03 core-02 kubelet-wrapper[31926]: E1030 22:23:03.524159   31926 desired_state_of_world_populator.go:285] Failed to add volume "mysql-persistent-storage" (specName: "pvc-9f273fbc") for pod "9f2ff89a-bdbf" to desiredStateOfWorld. err=failed to get Plugin from volumeSpec for volume "pvc-9f273fbc" err=no volume plugin matched
</span></code></pre></div></div>
<p>If you encounter this, just <strong>restart the Kubelet process</strong>, as described in the <strong><a href="/docs/rook/v0.6/kubernetes.html#restart-kubelet">Restart Kubelet</a></strong> section of the Rook deployment guide.</p>

<h3 id="volume-creation">Volume Creation</h3>
<p>The volume must first be created in the Rook cluster and then bound to a volume claim before it can be mounted to a pod.
Let’s confirm that with the following commands and their output:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>kubectl get pv
<span class="go">NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS     CLAIM                    STORAGECLASS   REASON    AGE
pvc-9f273fbc-bdbf-11e7-bc4c-001c428b9fc8   20Gi       RWO           Delete          Bound      default/mysql-pv-claim   rook-block               25m

</span><span class="gp">&gt;</span><span class="w"> </span>kubectl get pvc
<span class="go">NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS   AGE
mysql-pv-claim   Bound     pvc-9f273fbc-bdbf-11e7-bc4c-001c428b9fc8   20Gi       RWO           rook-block     25m
</span></code></pre></div></div>
<p>Both your volume and its claim should be in the <code class="language-plaintext highlighter-rouge">Bound</code> status.
If one or neither of them is not in the <code class="language-plaintext highlighter-rouge">Bound</code> status, then look for details of the issue in the <code class="language-plaintext highlighter-rouge">rook-operator</code> logs:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl -n rook-system logs `kubectl -n rook-system -l app=rook-operator get pods -o jsonpath='{.items[*].metadata.name}'`
</span></code></pre></div></div>
<p>If the volume is failing to be created, there should be details in the log output, especially those tagged with <code class="language-plaintext highlighter-rouge">op-provisioner</code>.</p>

<h3 id="volume-mounting">Volume Mounting</h3>
<p>The final step in preparing Rook storage for your pod is for the <code class="language-plaintext highlighter-rouge">rook-agent</code> pod to mount and format it.
If all the preceding sections have been successful or inconclusive, then take a look at the <code class="language-plaintext highlighter-rouge">rook-agent</code> pod logs for further clues.
You can determine which <code class="language-plaintext highlighter-rouge">rook-agent</code> is running on the same node that your pod is scheduled on by using the <code class="language-plaintext highlighter-rouge">-o wide</code> output, then you can get the logs for that <code class="language-plaintext highlighter-rouge">rook-agent</code> pod similar to the example below:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-system get pod <span class="nt">-o</span> wide
<span class="go">NAME                             READY     STATUS    RESTARTS   AGE       IP             NODE
rook-agent-h6scx                 1/1       Running   0          9m        172.17.8.102   172.17.8.102
rook-agent-mp7tn                 1/1       Running   0          9m        172.17.8.101   172.17.8.101
rook-operator-2203999069-3tb68   1/1       Running   0          9m        10.32.0.7      172.17.8.101

</span><span class="gp">&gt;</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook-system logs rook-agent-h6scx
<span class="go">2017-10-30 23:07:06.984108 I | rook: starting Rook v0.5.0-241.g48ce6de.dirty with arguments '/usr/local/bin/rook agent'
</span><span class="c">...
</span></code></pre></div></div>

<p>In the <code class="language-plaintext highlighter-rouge">rook-agent</code> pod logs, you may see a snippet similar to the following:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Failed to complete rbd: signal: interrupt.
</span></code></pre></div></div>
<p>In this case, the agent waited for the <code class="language-plaintext highlighter-rouge">rbd</code> command but it did not finish in a timely manner so the agent gave up and stopped it.
This can happen for multiple reasons, but using <code class="language-plaintext highlighter-rouge">dmesg</code> will likely give you insight into the root cause.
If <code class="language-plaintext highlighter-rouge">dmesg</code> shows something similar to below, then it means you have an old kernel that can’t talk to the cluster:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">libceph: mon2 10.205.92.13:6790 feature set mismatch, my 4a042a42 &lt; server's 2004a042a42, missing 20000000000
</span></code></pre></div></div>
<p>If <code class="language-plaintext highlighter-rouge">uname -a</code> shows that you have a kernel version older than <code class="language-plaintext highlighter-rouge">3.15</code>, you’ll need to perform <strong>one</strong> of the following:</p>
<ul>
  <li>Disable some Ceph features by starting the <a href="/docs/rook/v0.6/toolbox.html">rook toolbox</a> and running <code class="language-plaintext highlighter-rouge">ceph osd crush tunables bobtail</code></li>
  <li>Upgrade your kernel to <code class="language-plaintext highlighter-rouge">3.15</code> or later.</li>
</ul>

<h3 id="filesystem-mounting">Filesystem Mounting</h3>

<p>In the <code class="language-plaintext highlighter-rouge">rook-agent</code> pod logs, you may see a snippet similar to the following:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">2017-11-07 00:04:37.808870 I | rook-flexdriver: WARNING: The node kernel version is 4.4.0-87-generic, which do not support multiple ceph filesystems. The kernel version has to be at least 4.7. If you have multiple ceph filesystems, the result could be inconsistent
</span></code></pre></div></div>

<p>This will happen in kernels with versions older than 4.7, where the option <code class="language-plaintext highlighter-rouge">mds_namespace</code> is not supported. This option is used to specify a filesystem namespace.</p>

<p>In this case, if there is only one filesystem in the Rook cluster, there should be no issues and the mount should succeed. If you have more than one filesystem, inconsistent results may arise and the filesystem mounted may not be the one you specified.</p>

<p>If the issue is still not resolved from the steps above, please come chat with us on the <strong>#general</strong> channel of our <a href="https://slack.rook.io/">Rook Slack</a>.
We want to help you get your storage working and learn from those lessons to prevent users in the future from seeing the same issue.</p>

<h1 id="cluster-failing-to-service-requests">Cluster failing to service requests</h1>

<h2 id="symptoms-1">Symptoms</h2>
<ul>
  <li>Execution of the <code class="language-plaintext highlighter-rouge">ceph</code> command hangs</li>
  <li>Execution of the <code class="language-plaintext highlighter-rouge">rookctl</code> command hangs</li>
  <li>PersistentVolumes are not being created</li>
  <li>Large amount of slow requests are blocking</li>
  <li>Large amount of stuck requests are blocking</li>
  <li>One or more MONs are restarting periodically</li>
</ul>

<h2 id="investigation">Investigation</h2>
<p>Create a <a href="/docs/rook/v0.6/toolbox.html">rook-tools pod</a> to investigate the current state of CEPH. Here is an example of what one might see. In this case the <code class="language-plaintext highlighter-rouge">ceph status</code> command would just hang so a CTRL-C needed to be send. The <code class="language-plaintext highlighter-rouge">rookctl status</code> command is able to give a good amount of detail. In some cases the rook-api pod needs to be restarted for <code class="language-plaintext highlighter-rouge">rookctl</code> to be able to gather information. If the rook-api is restarted, then the rook-tools pod should be restarted also.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook <span class="nb">exec</span> <span class="nt">-it</span> rook-tools bash
<span class="gp">root@rook-tools:/#</span><span class="w"> </span>ceph status
<span class="go">^CCluster connection interrupted or timed out
</span><span class="gp">root@rook-tools:/#</span><span class="w"> </span>rookctl status
<span class="go">OVERALL STATUS: ERROR

SUMMARY:
SEVERITY   NAME              MESSAGE
</span><span class="gp">WARNING    REQUEST_SLOW      1664 slow requests are blocked &gt;</span><span class="w"> </span>32 sec
<span class="gp">ERROR      REQUEST_STUCK     102722 stuck requests are blocked &gt;</span><span class="w"> </span>4096 sec
<span class="gp">WARNING    TOO_MANY_PGS      too many PGs per OSD (323 &gt;</span><span class="w"> </span>max 300<span class="o">)</span>
<span class="go">WARNING    OSD_DOWN          1 osds down
WARNING    OSD_HOST_DOWN     1 host (1 osds) down
WARNING    PG_AVAILABILITY   Reduced data availability: 415 pgs stale
WARNING    PG_DEGRADED       Degraded data redundancy: 190/958 objects degraded (19.833%), 53 pgs unclean, 53 pgs degraded, 53 pgs undersized

USAGE:
TOTAL        USED        DATA       AVAILABLE
755.27 GiB   65.29 GiB   1.12 GiB   689.98 GiB

MONITORS:
NAME              ADDRESS               IN QUORUM   STATUS
rook-ceph-mon75   172.18.0.70:6790/0    true        OK
rook-ceph-mon21   172.18.0.245:6790/0   true        OK
rook-ceph-mon65   172.18.0.246:6790/0   true        OK

MGRs:
NAME             STATUS
rook-ceph-mgr0   Active

OSDs:
TOTAL     UP        IN        FULL      NEAR FULL
4         1         2         false     false

PLACEMENT GROUPS (600 total):
STATE                        COUNT
active+clean                 132
active+undersized+degraded   53
stale+active+clean           415
</span></code></pre></div></div>

<p>Another indication is when one or more of the MON pods restart frequently. Note the ‘mon107’ that has only been up for 16 minutes in the following output.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> rook get all <span class="nt">-o</span> wide <span class="nt">--show-all</span>
<span class="go">NAME                                 READY     STATUS    RESTARTS   AGE       IP               NODE
po/rook-api-41429188-x9l2r           1/1       Running   0          17h       192.168.1.187    k8-host-0401
po/rook-ceph-mgr0-2487684371-gzlbq   1/1       Running   0          17h       192.168.224.46   k8-host-0402
po/rook-ceph-mon107-p74rj            1/1       Running   0          16m       192.168.224.28   k8-host-0402
rook-ceph-mon1-56fgm                 1/1       Running   0          2d        192.168.91.135   k8-host-0404
rook-ceph-mon2-rlxcd                 1/1       Running   0          2d        192.168.123.33   k8-host-0403
rook-ceph-osd-bg2vj                  1/1       Running   0          2d        192.168.91.177   k8-host-0404
rook-ceph-osd-mwxdm                  1/1       Running   0          2d        192.168.123.31   k8-host-0403
</span></code></pre></div></div>

<h2 id="solution">Solution</h2>
<p>What is happening here is that the MON pods are restarting and one or more of the CEPH daemons are not getting configured with the proper cluster information. This is commonly the result of not specifying a value for <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> in your Cluster CRD.</p>

<p>The <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> setting specifies a path on the local host for the CEPH daemons to store configuration and data. Setting this to a path like <code class="language-plaintext highlighter-rouge">/var/lib/rook</code>, reapplying your Cluster CRD and restarting all the CEPH daemons (MON, MGR, OSD, RGW) should solve this problem. After the CEPH daemons have been restarted, it is advisable to restart the rook-api and <a href="/docs/rook/v0.6/toolbox.html">rook-tool Pods</a>.</p>

<h1 id="only-a-single-monitor-pod-starts-after-redeploy">Only a single monitor pod starts after redeploy</h1>

<h2 id="symptoms-2">Symptoms</h2>
<ul>
  <li>After tearing down a working cluster to redeploy a new cluster, the new cluster fails to start</li>
  <li>Rook operator is running</li>
  <li>Only a partial number of the MON daemons are created and are failing</li>
  <li>If the mons started, the OSD pods are failing</li>
</ul>

<h2 id="investigation-1">Investigation</h2>
<p>When attempting to reinstall Rook, the rook-operator pod gets started successfully and then the cluster CRD is then loaded. The rook-operator only starts up a single MON (possibly on rare occasion a second MON may be started) and just hangs. Looking at the log output of the rook-operator the last operation that was occuring was a <code class="language-plaintext highlighter-rouge">ceph mon_status</code>.</p>

<p>Looking at the log or the termination status for the <code class="language-plaintext highlighter-rouge">mon</code> pod, you will see a message indicating the keyring does not match from a previous deployment.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># the mon pod is in a crash loop backoff state
$ kubectl -n rook get pod
NAME                   READY     STATUS             RESTARTS   AGE
rook-ceph-mon0-r8tbl   0/1       CrashLoopBackOff   2          47s

# the pod shows a termination status that the keyring does not match the existing keyring
$ kubectl -n rook describe pod -l mon=rook-ceph-mon0
...
    Last State:		Terminated
      Reason:		Error
      Message:		The keyring does not match the existing keyring in /var/lib/rook/rook-ceph-mon0/data/keyring. 
                    You may need to delete the contents of dataDirHostPath on the host from a previous deployment.
...
</code></pre></div></div>

<p>If your cluster is larger than a couple nodes, you may get lucky enough that the monitors were able to start and form quorum. However, now the OSDs pods may fail to start due to state
from a previous deployment. Looking at the OSD pod logs you will see an error about the file already existing.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ kubectl -n rook logs rook-ceph-osd-fl8fs
...
2017-10-31 20:13:11.187106 I | mkfs-osd0: 2017-10-31 20:13:11.186992 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _read_fsid unparsable uuid 
2017-10-31 20:13:11.187208 I | mkfs-osd0: 2017-10-31 20:13:11.187026 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _setup_block_symlink_or_file failed to create block symlink to /dev/disk/by-partuuid/651153ba-2dfc-4231-ba06-94759e5ba273: (17) File exists
2017-10-31 20:13:11.187233 I | mkfs-osd0: 2017-10-31 20:13:11.187038 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) mkfs failed, (17) File exists
2017-10-31 20:13:11.187254 I | mkfs-osd0: 2017-10-31 20:13:11.187042 7f0059d62e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (17) File exists
2017-10-31 20:13:11.187275 I | mkfs-osd0: 2017-10-31 20:13:11.187121 7f0059d62e00 -1  ** ERROR: error creating empty object store in /var/lib/rook/osd0: (17) File exists
</code></pre></div></div>

<h2 id="solution-1">Solution</h2>
<p>This is a common problem reinitializing the Rook cluster when the local directory used for persistence has <strong>not</strong> been purged. 
This directory is the <code class="language-plaintext highlighter-rouge">dataDirHostPath</code> setting in the cluster CRD and is typically set to <code class="language-plaintext highlighter-rouge">/var/lib/rook</code>. 
To fix the issue you will need to delete all components of Rook and then delete the contents of <code class="language-plaintext highlighter-rouge">/var/lib/rook</code> (or the directory specified by <code class="language-plaintext highlighter-rouge">dataDirHostPath</code>) on each of the hosts in the cluster. 
Then when the cluster CRD is applied to start a new cluster, the rook-operator should start all the pods as expected.</p>

    </div>
  </div>
</div>

<script>
  var menu = [];
  var BASE_PATH = "";

  function add(name, url, isChild, current) {
    var item = { name: name, url: url, current: current };
    var container = menu;
    if (isChild && menu.length > 0) {
      menu[menu.length-1].children = menu[menu.length-1].children || [];
      container = menu[menu.length-1].children;
      if (current) {
        menu[menu.length-1].childCurrent = true;
      }
    }
    container.push(item);
  }

  
    add(
      "Rook",
      "/docs/rook/v0.6/",
      false,
      false
    );
  
    add(
      "Kubernetes",
      "/docs/rook/v0.6/kubernetes.html",
      false,
      false
    );
  
    add(
      "Prerequisites",
      "/docs/rook/v0.6/k8s-pre-reqs.html",
      true,
      false
    );
  
    add(
      "Block Storage",
      "/docs/rook/v0.6/k8s-block.html",
      true,
      false
    );
  
    add(
      "Object Storage",
      "/docs/rook/v0.6/k8s-object.html",
      true,
      false
    );
  
    add(
      "Shared File System",
      "/docs/rook/v0.6/k8s-filesystem.html",
      true,
      false
    );
  
    add(
      "Monitoring",
      "/docs/rook/v0.6/k8s-monitoring.html",
      true,
      false
    );
  
    add(
      "Pool CRD",
      "/docs/rook/v0.6/pool-crd.html",
      true,
      false
    );
  
    add(
      "Cluster CRD",
      "/docs/rook/v0.6/cluster-crd.html",
      true,
      false
    );
  
    add(
      "Object Store CRD",
      "/docs/rook/v0.6/object-store-crd.html",
      true,
      false
    );
  
    add(
      "File System CRD",
      "/docs/rook/v0.6/filesystem-crd.html",
      true,
      false
    );
  
    add(
      "Helm Chart",
      "/docs/rook/v0.6/helm-operator.html",
      true,
      false
    );
  
    add(
      "Upgrading Rook",
      "/docs/rook/v0.6/upgrade.html",
      true,
      false
    );
  
    add(
      "Rook Client",
      "/docs/rook/v0.6/client.html",
      false,
      false
    );
  
    add(
      "Contributing",
      "/docs/rook/v0.6/development-flow.html",
      false,
      false
    );
  
    add(
      "Toolbox",
      "/docs/rook/v0.6/toolbox.html",
      false,
      false
    );
  
    add(
      "Advanced Configuration",
      "/docs/rook/v0.6/advanced-configuration.html",
      true,
      false
    );
  
    add(
      "Common Problems",
      "/docs/rook/v0.6/common-problems.html",
      false,
      true
    );
  

  function getEntry(item) {
    var itemDom = document.createElement('li');

    if (item.current) {
      itemDom.innerHTML = item.name;
      itemDom.classList.add('current');
    } else {
      itemDom.innerHTML = '<a href="' + item.url + '">' + item.name + '</a>';
    }

    return itemDom;
  }

  // Flush css changes as explained in: https://stackoverflow.com/a/34726346
  // and more completely: https://stackoverflow.com/a/6956049
  function flushCss(element) {
    element.offsetHeight;
  }

  function addArrow(itemDom) {
    var MAIN_ITEM_HEIGHT = 24;
    var BOTTOM_PADDING = 20;
    var arrowDom = document.createElement('a');
    arrowDom.classList.add('arrow');
    arrowDom.innerHTML = '<img src="' + BASE_PATH + '/images/arrow.svg" />';
    arrowDom.onclick = function(itemDom) {
      return function () {
        // Calculated full height of the opened list
        var fullHeight = MAIN_ITEM_HEIGHT + BOTTOM_PADDING + itemDom.lastChild.clientHeight + 'px';

        itemDom.classList.toggle('open');

        if (itemDom.classList.contains('open')) {
          itemDom.style.height = fullHeight;
        } else {
          // If the list height is auto we have to set it to fullHeight
          // without tranistion before we shrink it to collapsed height
          if (itemDom.style.height === 'auto') {
            itemDom.style.transition = 'none';
            itemDom.style.height = fullHeight;
            flushCss(itemDom);
            itemDom.style.transition = '';
          }
          itemDom.style.height = MAIN_ITEM_HEIGHT + 'px';
        }

        return false;
      };
    }(itemDom);
    itemDom.appendChild(arrowDom);

    if ((item.current && item.children) || item.childCurrent) {
      itemDom.classList.add('open');
      itemDom.style.height = 'auto';
    } else {
      itemDom.style.height = MAIN_ITEM_HEIGHT + 'px';
    }
  }

  var menuDom = document.getElementById('docs-ul');
  for (var i = 0; i < menu.length; i++) {
    var item = menu[i];
    var itemDom = getEntry(item);

    if (item.childCurrent) {
      itemDom.classList.add('childCurrent');
    }

    if (item.children) {
      addArrow(itemDom);
      itemDom.classList.add('children');
      var children = document.createElement('ul');
      for (var j = 0; j < item.children.length; j++) {
        children.appendChild(getEntry(item.children[j]));
      }
      itemDom.appendChild(children);
    }
    menuDom.appendChild(itemDom);
  }
</script>
</div></main>
    <footer id="footer" aria-label="Footer">
  <div class="top">
    <a href="//www.cncf.io">
      <img
        class="cncf"
        src="/images/cncf.png"
        srcset="/images/cncf@2x.png 2x, /images/cncf@3x.png 3x" />
    </a>
    <p>We are a Cloud Native Computing Foundation graduated project.</p>
  </div>
  <div class="middle">
    <div class="grid-center">
      <div class="col_sm-12">
        <span>Getting Started</span>
        <a href="//github.com/rook/rook">GitHub</a>
        <a href="/docs/rook/v1.9/">Documentation</a>
        <a href="//github.com/rook/rook/blob/master/CONTRIBUTING.md#how-to-contribute">How to Contribute</a>
      </div>
      <div class="col_sm-12">
        <span>Community</span>
        <a href="//slack.rook.io/">Slack</a>
        <a href="//twitter.com/rook_io">Twitter</a>
        <a href="//groups.google.com/forum/#!forum/rook-dev">Forum</a>
        <a href="//blog.rook.io/">Blog</a>
      </div>
      <div class="col_sm-12">
        <span>Contact</span>
        <a href="mailto:cncf-rook-info@lists.cncf.io">Email</a>
        <a href="//github.com/rook/rook/issues">Feature request</a>
      </div>
      <div class="col_sm-12">
        <span>Top Contributors</span>
        <a href="//cloudical.io/">Cloudical</a>
        <a href="//cybozu.com">Cybozu, Inc</a>
        <a href="//www.redhat.com">Red Hat</a>
        <a href="//www.suse.com/">SUSE</a>
        <a href="//upbound.io">Upbound</a>
      </div>
    </div>
  </div>
  <div class="bottom">
    <div class="grid-center">
      <div class="col-8">
        <a class="logo" href="/">
          <img src="/images/rook-logo-small.svg" alt="rook.io" />
        </a>
        <p>
          &#169; Rook Authors 2022. Documentation distributed under
          <a href="https://creativecommons.org/licenses/by/4.0">CC-BY-4.0</a>.
        </p>
        <p>
          &#169; 2022 The Linux Foundation. All rights reserved. The Linux Foundation has
          registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our
          <a href="https://www.linuxfoundation.org/trademark-usage/">Trademark Usage</a> page.
        </p>
      </div>
    </div>
  </div>
</footer>


  <script src="/js/anchor.js"></script>
  <script>
    anchors.options = {
      placement: 'right',
      icon: '#',
    }

    document.addEventListener('DOMContentLoaded', function(event) {
      anchors.add('.docs-text h1, .docs-text h2, .docs-text h3, .docs-text h4, .docs-text h5, .docs-text h6');
    });
  </script>




    
  </body>
</html>
