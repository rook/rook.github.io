<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Rook Ceph Documentation"><meta name=author content="Rook Authors"><link href=https://rook.io/v1.14/Troubleshooting/ceph-common-issues/ rel=canonical><link href=../common-issues/ rel=prev><link href=../ceph-csi-common-issues/ rel=next><link rel=icon href=https://rook.io/images/favicon_192x192.png><meta name=generator content="mkdocs-1.6.0, mkdocs-material-9.5.29"><title>Ceph Common Issues - Rook Ceph Documentation</title><link rel=stylesheet href=../../assets/stylesheets/main.76a95c52.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=rook-blue data-md-color-accent=deep-orange> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#topics class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> <aside class="md-banner md-banner--warning"> <div class="md-banner__inner md-grid md-typeset"> This documentation is not for the latest stable version of Rook. <a href=../../../latest-release/ > <strong>Click here for the latest release documentation.</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=outdated]"),outdated=__md_get("__outdated",sessionStorage);!0===outdated&&el&&(el.hidden=!1)</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Rook Ceph Documentation" class="md-header__button md-logo" aria-label="Rook Ceph Documentation" data-md-component=logo> <img src=https://rook.io/images/rook-logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Rook Ceph Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Ceph Common Issues </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=rook-blue data-md-color-accent=deep-orange aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=rook-blue data-md-color-accent=red aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/rook/rook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../Getting-Started/intro/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../../Helm-Charts/helm-charts/ class=md-tabs__link> Helm Charts </a> </li> <li class=md-tabs__item> <a href=../../Storage-Configuration/Block-Storage-RBD/block-storage/ class=md-tabs__link> Storage Configuration </a> </li> <li class=md-tabs__item> <a href=../../CRDs/Cluster/ceph-cluster-crd/ class=md-tabs__link> Custom Resources </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../kubectl-plugin/ class=md-tabs__link> Troubleshooting </a> </li> <li class=md-tabs__item> <a href=../../Upgrade/health-verification/ class=md-tabs__link> Upgrade </a> </li> <li class=md-tabs__item> <a href=../../Contributing/development-flow/ class=md-tabs__link> Contributing </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Rook Ceph Documentation" class="md-nav__button md-logo" aria-label="Rook Ceph Documentation" data-md-component=logo> <img src=https://rook.io/images/rook-logo.svg alt=logo> </a> Rook Ceph Documentation </label> <div class=md-nav__source> <a href=https://github.com/rook/rook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1> <label class=md-nav__link for=__nav_1 id=__nav_1_label tabindex=0> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Getting Started </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Getting-Started/intro/ class=md-nav__link> <span class=md-ellipsis> Rook </span> </a> </li> <li class=md-nav__item> <a href=../../Getting-Started/glossary/ class=md-nav__link> <span class=md-ellipsis> Glossary </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1_3> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex=0> <span class=md-ellipsis> Prerequisites </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> Prerequisites </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Getting-Started/Prerequisites/prerequisites/ class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=../../Getting-Started/Prerequisites/authenticated-registry/ class=md-nav__link> <span class=md-ellipsis> Authenticated Container Registries </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../Getting-Started/quickstart/ class=md-nav__link> <span class=md-ellipsis> Quickstart </span> </a> </li> <li class=md-nav__item> <a href=../../Getting-Started/storage-architecture/ class=md-nav__link> <span class=md-ellipsis> Storage Architecture </span> </a> </li> <li class=md-nav__item> <a href=../../Getting-Started/example-configurations/ class=md-nav__link> <span class=md-ellipsis> Example Configurations </span> </a> </li> <li class=md-nav__item> <a href=../../Getting-Started/ceph-openshift/ class=md-nav__link> <span class=md-ellipsis> OpenShift </span> </a> </li> <li class=md-nav__item> <a href=../../Getting-Started/ceph-teardown/ class=md-nav__link> <span class=md-ellipsis> Cleanup </span> </a> </li> <li class=md-nav__item> <a href=../../Getting-Started/release-cycle/ class=md-nav__link> <span class=md-ellipsis> Release Cycle </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Helm Charts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Helm Charts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Helm-Charts/helm-charts/ class=md-nav__link> <span class=md-ellipsis> Helm Charts Overview </span> </a> </li> <li class=md-nav__item> <a href=../../Helm-Charts/operator-chart/ class=md-nav__link> <span class=md-ellipsis> Ceph Operator Helm Chart </span> </a> </li> <li class=md-nav__item> <a href=../../Helm-Charts/ceph-cluster-chart/ class=md-nav__link> <span class=md-ellipsis> Ceph Cluster Helm Chart </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Storage Configuration </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Storage Configuration </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> Block Storage (RBD) </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> Block Storage (RBD) </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Storage-Configuration/Block-Storage-RBD/block-storage/ class=md-nav__link> <span class=md-ellipsis> Block Storage Overview </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Block-Storage-RBD/rbd-mirroring/ class=md-nav__link> <span class=md-ellipsis> RBD Mirroring </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Block-Storage-RBD/rbd-async-disaster-recovery-failover-failback/ class=md-nav__link> <span class=md-ellipsis> RBD Asynchronous DR Failover and Failback </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex=0> <span class=md-ellipsis> Shared Filesystem (CephFS) </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Shared Filesystem (CephFS) </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage/ class=md-nav__link> <span class=md-ellipsis> Filesystem Storage Overview </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Shared-Filesystem-CephFS/filesystem-mirroring/ class=md-nav__link> <span class=md-ellipsis> Filesystem Mirroring </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex=0> <span class=md-ellipsis> Object Storage (RGW) </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Object Storage (RGW) </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Storage-Configuration/Object-Storage-RGW/object-storage/ class=md-nav__link> <span class=md-ellipsis> Object Storage Overview </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim/ class=md-nav__link> <span class=md-ellipsis> Bucket Claim </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-notifications/ class=md-nav__link> <span class=md-ellipsis> Object Bucket Notifications </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Object-Storage-RGW/ceph-object-multisite/ class=md-nav__link> <span class=md-ellipsis> Object Store Multisite </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Object-Storage-RGW/cosi/ class=md-nav__link> <span class=md-ellipsis> Container Object Storage Interface (COSI) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_4> <label class=md-nav__link for=__nav_3_4 id=__nav_3_4_label tabindex=0> <span class=md-ellipsis> NFS </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_3_4> <span class="md-nav__icon md-icon"></span> NFS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Storage-Configuration/NFS/nfs/ class=md-nav__link> <span class=md-ellipsis> NFS Storage Overview </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/NFS/nfs-csi-driver/ class=md-nav__link> <span class=md-ellipsis> CSI provisioner and driver </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/NFS/nfs-security/ class=md-nav__link> <span class=md-ellipsis> Security </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/NFS/nfs-advanced/ class=md-nav__link> <span class=md-ellipsis> Advanced configuration </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_5> <label class=md-nav__link for=__nav_3_5 id=__nav_3_5_label tabindex=0> <span class=md-ellipsis> Monitoring </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_5_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5> <span class="md-nav__icon md-icon"></span> Monitoring </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Storage-Configuration/Monitoring/ceph-dashboard/ class=md-nav__link> <span class=md-ellipsis> Ceph Dashboard </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Monitoring/ceph-monitoring/ class=md-nav__link> <span class=md-ellipsis> Prometheus Monitoring </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_6> <label class=md-nav__link for=__nav_3_6 id=__nav_3_6_label tabindex=0> <span class=md-ellipsis> Ceph CSI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_6_label aria-expanded=false> <label class=md-nav__title for=__nav_3_6> <span class="md-nav__icon md-icon"></span> Ceph CSI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Storage-Configuration/Ceph-CSI/ceph-csi-drivers/ class=md-nav__link> <span class=md-ellipsis> Ceph CSI Drivers </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Ceph-CSI/ceph-csi-snapshot/ class=md-nav__link> <span class=md-ellipsis> Snapshots </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Ceph-CSI/ceph-csi-volume-clone/ class=md-nav__link> <span class=md-ellipsis> Volume clone </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Ceph-CSI/custom-images/ class=md-nav__link> <span class=md-ellipsis> Custom Images </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/ceph-teardown/ class=md-nav__link> <span class=md-ellipsis> Cleanup </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> Advanced </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> Advanced </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Storage-Configuration/Advanced/ceph-configuration/ class=md-nav__link> <span class=md-ellipsis> Ceph Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Advanced/configuration/ class=md-nav__link> <span class=md-ellipsis> Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Advanced/key-management-system/ class=md-nav__link> <span class=md-ellipsis> Key Management System </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Advanced/ceph-osd-mgmt/ class=md-nav__link> <span class=md-ellipsis> Ceph OSD Management </span> </a> </li> <li class=md-nav__item> <a href=../../Storage-Configuration/Advanced/ceph-mon-health/ class=md-nav__link> <span class=md-ellipsis> Monitor Health </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Custom Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Custom Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_1> <label class=md-nav__link for=__nav_4_1 id=__nav_4_1_label tabindex=0> <span class=md-ellipsis> Cluster Configuration </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_1_label aria-expanded=false> <label class=md-nav__title for=__nav_4_1> <span class="md-nav__icon md-icon"></span> Cluster Configuration </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CRDs/Cluster/ceph-cluster-crd/ class=md-nav__link> <span class=md-ellipsis> CephCluster CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Cluster/host-cluster/ class=md-nav__link> <span class=md-ellipsis> Host Storage Cluster </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Cluster/pvc-cluster/ class=md-nav__link> <span class=md-ellipsis> PVC Storage Cluster </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Cluster/stretch-cluster/ class=md-nav__link> <span class=md-ellipsis> Stretch Storage Cluster </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_1_5> <label class=md-nav__link for=__nav_4_1_5 id=__nav_4_1_5_label tabindex=0> <span class=md-ellipsis> External cluster </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_1_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_1_5> <span class="md-nav__icon md-icon"></span> External cluster </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CRDs/Cluster/external-cluster/external-cluster/ class=md-nav__link> <span class=md-ellipsis> External Storage Cluster </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Cluster/external-cluster/topology-for-external-mode/ class=md-nav__link> <span class=md-ellipsis> Topology-Based Provisioning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../CRDs/Cluster/network-providers/ class=md-nav__link> <span class=md-ellipsis> Network Providers </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> Block Storage </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Block Storage </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CRDs/Block-Storage/ceph-block-pool-crd/ class=md-nav__link> <span class=md-ellipsis> CephBlockPool CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Block-Storage/ceph-block-pool-rados-namespace-crd/ class=md-nav__link> <span class=md-ellipsis> CephBlockPoolRados Namespace CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Block-Storage/ceph-rbd-mirror-crd/ class=md-nav__link> <span class=md-ellipsis> CephRBDMirror CRD </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class=md-ellipsis> Shared Filesystem </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> Shared Filesystem </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CRDs/Shared-Filesystem/ceph-filesystem-crd/ class=md-nav__link> <span class=md-ellipsis> CephFilesystem CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Shared-Filesystem/ceph-fs-mirror-crd/ class=md-nav__link> <span class=md-ellipsis> CephFilesystemMirror CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Shared-Filesystem/ceph-fs-subvolumegroup-crd/ class=md-nav__link> <span class=md-ellipsis> FilesystemSubVolumeGroup CRD </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex=0> <span class=md-ellipsis> Object Storage </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> Object Storage </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CRDs/Object-Storage/ceph-object-store-crd/ class=md-nav__link> <span class=md-ellipsis> CephObjectStore CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Object-Storage/ceph-object-store-user-crd/ class=md-nav__link> <span class=md-ellipsis> CephObjectStoreUser CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Object-Storage/ceph-object-realm-crd/ class=md-nav__link> <span class=md-ellipsis> CephObjectRealm CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Object-Storage/ceph-object-zonegroup-crd/ class=md-nav__link> <span class=md-ellipsis> CephObjectZoneGroup CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/Object-Storage/ceph-object-zone-crd/ class=md-nav__link> <span class=md-ellipsis> CephObjectZone CRD </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../CRDs/ceph-client-crd/ class=md-nav__link> <span class=md-ellipsis> CephClient CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/ceph-nfs-crd/ class=md-nav__link> <span class=md-ellipsis> CephNFS CRD </span> </a> </li> <li class=md-nav__item> <a href=../../CRDs/specification/ class=md-nav__link> <span class=md-ellipsis> Specification </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class=md-ellipsis> Troubleshooting </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Troubleshooting </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../kubectl-plugin/ class=md-nav__link> <span class=md-ellipsis> kubectl Plugin </span> </a> </li> <li class=md-nav__item> <a href=../ceph-toolbox/ class=md-nav__link> <span class=md-ellipsis> Toolbox </span> </a> </li> <li class=md-nav__item> <a href=../common-issues/ class=md-nav__link> <span class=md-ellipsis> Common Issues </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Ceph Common Issues </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Ceph Common Issues </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#topics class=md-nav__link> <span class=md-ellipsis> Topics </span> </a> </li> <li class=md-nav__item> <a href=#troubleshooting-techniques class=md-nav__link> <span class=md-ellipsis> Troubleshooting Techniques </span> </a> <nav class=md-nav aria-label="Troubleshooting Techniques"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#ceph-tools class=md-nav__link> <span class=md-ellipsis> Ceph Tools </span> </a> <nav class=md-nav aria-label="Ceph Tools"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#tools-in-the-rook-toolbox class=md-nav__link> <span class=md-ellipsis> Tools in the Rook Toolbox </span> </a> </li> <li class=md-nav__item> <a href=#ceph-commands class=md-nav__link> <span class=md-ellipsis> Ceph Commands </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cluster-failing-to-service-requests class=md-nav__link> <span class=md-ellipsis> Cluster failing to service requests </span> </a> <nav class=md-nav aria-label="Cluster failing to service requests"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#monitors-are-the-only-pods-running class=md-nav__link> <span class=md-ellipsis> Monitors are the only pods running </span> </a> <nav class=md-nav aria-label="Monitors are the only pods running"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_1 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_1 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> <nav class=md-nav aria-label=Investigation> <ul class=md-nav__list> <li class=md-nav__item> <a href=#operator-fails-to-connect-to-the-mon class=md-nav__link> <span class=md-ellipsis> Operator fails to connect to the mon </span> </a> </li> <li class=md-nav__item> <a href=#solution_1 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> <li class=md-nav__item> <a href=#failing-mon-pod class=md-nav__link> <span class=md-ellipsis> Failing mon pod </span> </a> </li> <li class=md-nav__item> <a href=#solution_2 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#pvcs-stay-in-pending-state class=md-nav__link> <span class=md-ellipsis> PVCs stay in pending state </span> </a> <nav class=md-nav aria-label="PVCs stay in pending state"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_2 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_2 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> <nav class=md-nav aria-label=Investigation> <ul class=md-nav__list> <li class=md-nav__item> <a href=#confirm-if-there-are-osds class=md-nav__link> <span class=md-ellipsis> Confirm if there are OSDs </span> </a> </li> <li class=md-nav__item> <a href=#osd-prepare-logs class=md-nav__link> <span class=md-ellipsis> OSD Prepare Logs </span> </a> </li> <li class=md-nav__item> <a href=#csi-driver class=md-nav__link> <span class=md-ellipsis> CSI Driver </span> </a> </li> <li class=md-nav__item> <a href=#operator-unresponsiveness class=md-nav__link> <span class=md-ellipsis> Operator unresponsiveness </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#solution_3 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#osd-pods-are-failing-to-start class=md-nav__link> <span class=md-ellipsis> OSD pods are failing to start </span> </a> <nav class=md-nav aria-label="OSD pods are failing to start"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_3 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_3 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution_4 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#osd-pods-are-not-created-on-my-devices class=md-nav__link> <span class=md-ellipsis> OSD pods are not created on my devices </span> </a> <nav class=md-nav aria-label="OSD pods are not created on my devices"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_4 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_4 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution_5 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#node-hangs-after-reboot class=md-nav__link> <span class=md-ellipsis> Node hangs after reboot </span> </a> <nav class=md-nav aria-label="Node hangs after reboot"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_5 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_5 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution_6 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47 class=md-nav__link> <span class=md-ellipsis> Using multiple shared filesystem (CephFS) is attempted on a kernel version older than 4.7 </span> </a> <nav class=md-nav aria-label="Using multiple shared filesystem (CephFS) is attempted on a kernel version older than 4.7"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_6 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#solution_7 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#set-debug-log-level-for-all-ceph-daemons class=md-nav__link> <span class=md-ellipsis> Set debug log level for all Ceph daemons </span> </a> </li> <li class=md-nav__item> <a href=#activate-log-to-file-for-a-particular-ceph-daemon class=md-nav__link> <span class=md-ellipsis> Activate log to file for a particular Ceph daemon </span> </a> </li> <li class=md-nav__item> <a href=#a-worker-node-using-rbd-devices-hangs-up class=md-nav__link> <span class=md-ellipsis> A worker node using RBD devices hangs up </span> </a> <nav class=md-nav aria-label="A worker node using RBD devices hangs up"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_7 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_6 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution_8 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#too-few-pgs-per-osd-warning-is-shown class=md-nav__link> <span class=md-ellipsis> Too few PGs per OSD warning is shown </span> </a> <nav class=md-nav aria-label="Too few PGs per OSD warning is shown"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_8 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#solution_9 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc class=md-nav__link> <span class=md-ellipsis> LVM metadata can be corrupted with OSD on LV-backed PVC </span> </a> <nav class=md-nav aria-label="LVM metadata can be corrupted with OSD on LV-backed PVC"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_9 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#solution_10 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#osd-prepare-job-fails-due-to-low-aio-max-nr-setting class=md-nav__link> <span class=md-ellipsis> OSD prepare job fails due to low aio-max-nr setting </span> </a> </li> <li class=md-nav__item> <a href=#unexpected-partitions-created class=md-nav__link> <span class=md-ellipsis> Unexpected partitions created </span> </a> <nav class=md-nav aria-label="Unexpected partitions created"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_10 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#solution_11 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> <nav class=md-nav aria-label=Solution> <ul class=md-nav__list> <li class=md-nav__item> <a href=#recover-from-corruption-v160-v167 class=md-nav__link> <span class=md-ellipsis> Recover from corruption (v1.6.0-v1.6.7) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#operator-environment-variables-are-ignored class=md-nav__link> <span class=md-ellipsis> Operator environment variables are ignored </span> </a> <nav class=md-nav aria-label="Operator environment variables are ignored"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_11 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_7 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution_12 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../ceph-csi-common-issues/ class=md-nav__link> <span class=md-ellipsis> CSI Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../openshift-common-issues/ class=md-nav__link> <span class=md-ellipsis> OpenShift Common Issues </span> </a> </li> <li class=md-nav__item> <a href=../disaster-recovery/ class=md-nav__link> <span class=md-ellipsis> Disaster Recovery </span> </a> </li> <li class=md-nav__item> <a href=../direct-tools/ class=md-nav__link> <span class=md-ellipsis> Direct Tools </span> </a> </li> <li class=md-nav__item> <a href=../performance-profiling/ class=md-nav__link> <span class=md-ellipsis> Performance Profiling </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Upgrade </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Upgrade </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Upgrade/health-verification/ class=md-nav__link> <span class=md-ellipsis> Health Verification </span> </a> </li> <li class=md-nav__item> <a href=../../Upgrade/rook-upgrade/ class=md-nav__link> <span class=md-ellipsis> Rook Upgrades </span> </a> </li> <li class=md-nav__item> <a href=../../Upgrade/ceph-upgrade/ class=md-nav__link> <span class=md-ellipsis> Ceph Upgrades </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class=md-ellipsis> Contributing </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Contributing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Contributing/development-flow/ class=md-nav__link> <span class=md-ellipsis> Development Flow </span> </a> </li> <li class=md-nav__item> <a href=../../Contributing/development-environment/ class=md-nav__link> <span class=md-ellipsis> Developer Environment </span> </a> </li> <li class=md-nav__item> <a href=../../Contributing/documentation/ class=md-nav__link> <span class=md-ellipsis> Documentation </span> </a> </li> <li class=md-nav__item> <a href=../../Contributing/ci-configuration/ class=md-nav__link> <span class=md-ellipsis> CI Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../Contributing/rook-test-framework/ class=md-nav__link> <span class=md-ellipsis> Rook Test Framework </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#topics class=md-nav__link> <span class=md-ellipsis> Topics </span> </a> </li> <li class=md-nav__item> <a href=#troubleshooting-techniques class=md-nav__link> <span class=md-ellipsis> Troubleshooting Techniques </span> </a> <nav class=md-nav aria-label="Troubleshooting Techniques"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#ceph-tools class=md-nav__link> <span class=md-ellipsis> Ceph Tools </span> </a> <nav class=md-nav aria-label="Ceph Tools"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#tools-in-the-rook-toolbox class=md-nav__link> <span class=md-ellipsis> Tools in the Rook Toolbox </span> </a> </li> <li class=md-nav__item> <a href=#ceph-commands class=md-nav__link> <span class=md-ellipsis> Ceph Commands </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cluster-failing-to-service-requests class=md-nav__link> <span class=md-ellipsis> Cluster failing to service requests </span> </a> <nav class=md-nav aria-label="Cluster failing to service requests"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#monitors-are-the-only-pods-running class=md-nav__link> <span class=md-ellipsis> Monitors are the only pods running </span> </a> <nav class=md-nav aria-label="Monitors are the only pods running"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_1 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_1 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> <nav class=md-nav aria-label=Investigation> <ul class=md-nav__list> <li class=md-nav__item> <a href=#operator-fails-to-connect-to-the-mon class=md-nav__link> <span class=md-ellipsis> Operator fails to connect to the mon </span> </a> </li> <li class=md-nav__item> <a href=#solution_1 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> <li class=md-nav__item> <a href=#failing-mon-pod class=md-nav__link> <span class=md-ellipsis> Failing mon pod </span> </a> </li> <li class=md-nav__item> <a href=#solution_2 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#pvcs-stay-in-pending-state class=md-nav__link> <span class=md-ellipsis> PVCs stay in pending state </span> </a> <nav class=md-nav aria-label="PVCs stay in pending state"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_2 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_2 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> <nav class=md-nav aria-label=Investigation> <ul class=md-nav__list> <li class=md-nav__item> <a href=#confirm-if-there-are-osds class=md-nav__link> <span class=md-ellipsis> Confirm if there are OSDs </span> </a> </li> <li class=md-nav__item> <a href=#osd-prepare-logs class=md-nav__link> <span class=md-ellipsis> OSD Prepare Logs </span> </a> </li> <li class=md-nav__item> <a href=#csi-driver class=md-nav__link> <span class=md-ellipsis> CSI Driver </span> </a> </li> <li class=md-nav__item> <a href=#operator-unresponsiveness class=md-nav__link> <span class=md-ellipsis> Operator unresponsiveness </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#solution_3 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#osd-pods-are-failing-to-start class=md-nav__link> <span class=md-ellipsis> OSD pods are failing to start </span> </a> <nav class=md-nav aria-label="OSD pods are failing to start"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_3 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_3 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution_4 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#osd-pods-are-not-created-on-my-devices class=md-nav__link> <span class=md-ellipsis> OSD pods are not created on my devices </span> </a> <nav class=md-nav aria-label="OSD pods are not created on my devices"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_4 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_4 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution_5 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#node-hangs-after-reboot class=md-nav__link> <span class=md-ellipsis> Node hangs after reboot </span> </a> <nav class=md-nav aria-label="Node hangs after reboot"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_5 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_5 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution_6 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47 class=md-nav__link> <span class=md-ellipsis> Using multiple shared filesystem (CephFS) is attempted on a kernel version older than 4.7 </span> </a> <nav class=md-nav aria-label="Using multiple shared filesystem (CephFS) is attempted on a kernel version older than 4.7"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_6 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#solution_7 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#set-debug-log-level-for-all-ceph-daemons class=md-nav__link> <span class=md-ellipsis> Set debug log level for all Ceph daemons </span> </a> </li> <li class=md-nav__item> <a href=#activate-log-to-file-for-a-particular-ceph-daemon class=md-nav__link> <span class=md-ellipsis> Activate log to file for a particular Ceph daemon </span> </a> </li> <li class=md-nav__item> <a href=#a-worker-node-using-rbd-devices-hangs-up class=md-nav__link> <span class=md-ellipsis> A worker node using RBD devices hangs up </span> </a> <nav class=md-nav aria-label="A worker node using RBD devices hangs up"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_7 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_6 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution_8 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#too-few-pgs-per-osd-warning-is-shown class=md-nav__link> <span class=md-ellipsis> Too few PGs per OSD warning is shown </span> </a> <nav class=md-nav aria-label="Too few PGs per OSD warning is shown"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_8 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#solution_9 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc class=md-nav__link> <span class=md-ellipsis> LVM metadata can be corrupted with OSD on LV-backed PVC </span> </a> <nav class=md-nav aria-label="LVM metadata can be corrupted with OSD on LV-backed PVC"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_9 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#solution_10 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#osd-prepare-job-fails-due-to-low-aio-max-nr-setting class=md-nav__link> <span class=md-ellipsis> OSD prepare job fails due to low aio-max-nr setting </span> </a> </li> <li class=md-nav__item> <a href=#unexpected-partitions-created class=md-nav__link> <span class=md-ellipsis> Unexpected partitions created </span> </a> <nav class=md-nav aria-label="Unexpected partitions created"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_10 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#solution_11 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> <nav class=md-nav aria-label=Solution> <ul class=md-nav__list> <li class=md-nav__item> <a href=#recover-from-corruption-v160-v167 class=md-nav__link> <span class=md-ellipsis> Recover from corruption (v1.6.0-v1.6.7) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#operator-environment-variables-are-ignored class=md-nav__link> <span class=md-ellipsis> Operator environment variables are ignored </span> </a> <nav class=md-nav aria-label="Operator environment variables are ignored"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#symptoms_11 class=md-nav__link> <span class=md-ellipsis> Symptoms </span> </a> </li> <li class=md-nav__item> <a href=#investigation_7 class=md-nav__link> <span class=md-ellipsis> Investigation </span> </a> </li> <li class=md-nav__item> <a href=#solution_12 class=md-nav__link> <span class=md-ellipsis> Solution </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>Ceph Common Issues</h1> <h2 id=topics>Topics<a class=headerlink href=#topics title="Permanent link">&para;</a></h2> <ul> <li><a href=#troubleshooting-techniques>Troubleshooting Techniques</a><ul> <li><a href=#ceph-tools>Ceph Tools</a><ul> <li><a href=#tools-in-the-rook-toolbox>Tools in the Rook Toolbox</a></li> <li><a href=#ceph-commands>Ceph Commands</a></li> </ul> </li> </ul> </li> <li><a href=#cluster-failing-to-service-requests>Cluster failing to service requests</a><ul> <li><a href=#symptoms>Symptoms</a></li> <li><a href=#investigation>Investigation</a></li> <li><a href=#solution>Solution</a></li> </ul> </li> <li><a href=#monitors-are-the-only-pods-running>Monitors are the only pods running</a><ul> <li><a href=#symptoms-1>Symptoms</a></li> <li><a href=#investigation-1>Investigation</a><ul> <li><a href=#operator-fails-to-connect-to-the-mon>Operator fails to connect to the mon</a></li> <li><a href=#solution-1>Solution</a></li> <li><a href=#failing-mon-pod>Failing mon pod</a></li> <li><a href=#solution-2>Solution</a></li> </ul> </li> </ul> </li> <li><a href=#pvcs-stay-in-pending-state>PVCs stay in pending state</a><ul> <li><a href=#symptoms-2>Symptoms</a></li> <li><a href=#investigation-2>Investigation</a><ul> <li><a href=#confirm-if-there-are-osds>Confirm if there are OSDs</a></li> <li><a href=#osd-prepare-logs>OSD Prepare Logs</a></li> <li><a href=#csi-driver>CSI Driver</a></li> <li><a href=#operator-unresponsiveness>Operator unresponsiveness</a></li> </ul> </li> <li><a href=#solution-3>Solution</a></li> </ul> </li> <li><a href=#osd-pods-are-failing-to-start>OSD pods are failing to start</a><ul> <li><a href=#symptoms-3>Symptoms</a></li> <li><a href=#investigation-3>Investigation</a></li> <li><a href=#solution-4>Solution</a></li> </ul> </li> <li><a href=#osd-pods-are-not-created-on-my-devices>OSD pods are not created on my devices</a><ul> <li><a href=#symptoms-4>Symptoms</a></li> <li><a href=#investigation-4>Investigation</a></li> <li><a href=#solution-5>Solution</a></li> </ul> </li> <li><a href=#node-hangs-after-reboot>Node hangs after reboot</a><ul> <li><a href=#symptoms-5>Symptoms</a></li> <li><a href=#investigation-5>Investigation</a></li> <li><a href=#solution-6>Solution</a></li> </ul> </li> <li><a href=#using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47>Using multiple shared filesystem (CephFS) is attempted on a kernel version older than 4.7</a><ul> <li><a href=#symptoms-6>Symptoms</a></li> <li><a href=#solution-7>Solution</a></li> </ul> </li> <li><a href=#set-debug-log-level-for-all-ceph-daemons>Set debug log level for all Ceph daemons</a></li> <li><a href=#activate-log-to-file-for-a-particular-ceph-daemon>Activate log to file for a particular Ceph daemon</a></li> <li><a href=#a-worker-node-using-rbd-devices-hangs-up>A worker node using RBD devices hangs up</a><ul> <li><a href=#symptoms-7>Symptoms</a></li> <li><a href=#investigation-6>Investigation</a></li> <li><a href=#solution-8>Solution</a></li> </ul> </li> <li><a href=#too-few-pgs-per-osd-warning-is-shown>Too few PGs per OSD warning is shown</a><ul> <li><a href=#symptoms-8>Symptoms</a></li> <li><a href=#solution-9>Solution</a></li> </ul> </li> <li><a href=#lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc>LVM metadata can be corrupted with OSD on LV-backed PVC</a><ul> <li><a href=#symptoms-9>Symptoms</a></li> <li><a href=#solution-10>Solution</a></li> </ul> </li> <li><a href=#osd-prepare-job-fails-due-to-low-aio-max-nr-setting>OSD prepare job fails due to low aio-max-nr setting</a></li> <li><a href=#unexpected-partitions-created>Unexpected partitions created</a><ul> <li><a href=#symptoms-10>Symptoms</a></li> <li><a href=#solution-11>Solution</a><ul> <li><a href=#recover-from-corruption-v160-v167>Recover from corruption (v1.6.0-v1.6.7)</a></li> </ul> </li> </ul> </li> <li><a href=#operator-environment-variables-are-ignored>Operator environment variables are ignored</a><ul> <li><a href=#symptoms-11>Symptoms</a></li> <li><a href=#investigation-7>Investigation</a></li> <li><a href=#solution-12>Solution</a></li> </ul> </li> </ul> <p>Many of these problem cases are hard to summarize down to a short phrase that adequately describes the problem. Each problem will start with a bulleted list of symptoms. Keep in mind that all symptoms may not apply depending on the configuration of Rook. If the majority of the symptoms are seen there is a fair chance you are experiencing that problem.</p> <p>If after trying the suggestions found on this page and the problem is not resolved, the Rook team is very happy to help you troubleshoot the issues in their Slack channel. Once you have <a href=https://slack.rook.io>registered for the Rook Slack</a>, proceed to the <code>#ceph</code> channel to ask for assistance.</p> <p>See also the <a href=../ceph-csi-common-issues/ >CSI Troubleshooting Guide</a>.</p> <h2 id=troubleshooting-techniques>Troubleshooting Techniques<a class=headerlink href=#troubleshooting-techniques title="Permanent link">&para;</a></h2> <p>There are two main categories of information you will need to investigate issues in the cluster:</p> <ol> <li>Kubernetes status and logs documented <a href=../common-issues/ >here</a></li> <li>Ceph cluster status (see upcoming <a href=#ceph-tools>Ceph tools</a> section)</li> </ol> <h3 id=ceph-tools>Ceph Tools<a class=headerlink href=#ceph-tools title="Permanent link">&para;</a></h3> <p>After you verify the basic health of the running pods, next you will want to run Ceph tools for status of the storage components. There are two ways to run the Ceph tools, either in the Rook toolbox or inside other Rook pods that are already running.</p> <ul> <li>Logs on a specific node to find why a PVC is failing to mount</li> <li>See the <a href=../../Storage-Configuration/Advanced/ceph-configuration/#log-collection>log collection topic</a> for a script that will help you gather the logs</li> <li>Other artifacts:<ul> <li>The monitors that are expected to be in quorum: <code>kubectl -n &lt;cluster-namespace&gt; get configmap rook-ceph-mon-endpoints -o yaml | grep data</code></li> </ul> </li> </ul> <h4 id=tools-in-the-rook-toolbox>Tools in the Rook Toolbox<a class=headerlink href=#tools-in-the-rook-toolbox title="Permanent link">&para;</a></h4> <p>The <a href=../ceph-toolbox/ >rook-ceph-tools pod</a> provides a simple environment to run Ceph tools. Once the pod is up and running, connect to the pod to execute Ceph commands to evaluate that current state of the cluster.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-1 name=__codelineno-0-1></a><span class=go>kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; -o jsonpath=&#39;{.items[*].metadata.name}&#39;) bash</span>
</code></pre></div></td></tr></table></div> <h4 id=ceph-commands>Ceph Commands<a class=headerlink href=#ceph-commands title="Permanent link">&para;</a></h4> <p>Here are some common commands to troubleshoot a Ceph cluster:</p> <ul> <li><code>ceph status</code></li> <li><code>ceph osd status</code></li> <li><code>ceph osd df</code></li> <li><code>ceph osd utilization</code></li> <li><code>ceph osd pool stats</code></li> <li><code>ceph osd tree</code></li> <li><code>ceph pg stat</code></li> </ul> <p>The first two status commands provide the overall cluster health. The normal state for cluster operations is HEALTH_OK, but will still function when the state is in a HEALTH_WARN state. If you are in a WARN state, then the cluster is in a condition that it may enter the HEALTH_ERROR state at which point <em>all</em> disk I/O operations are halted. If a HEALTH_WARN state is observed, then one should take action to prevent the cluster from halting when it enters the HEALTH_ERROR state.</p> <p>There are many Ceph sub-commands to look at and manipulate Ceph objects, well beyond the scope this document. See the <a href=https://docs.ceph.com/ >Ceph documentation</a> for more details of gathering information about the health of the cluster. In addition, there are other helpful hints and some best practices located in the <a href=../../Storage-Configuration/Advanced/ceph-configuration/ >Advanced Configuration section</a>. Of particular note, there are scripts for collecting logs and gathering OSD information there.</p> <h2 id=cluster-failing-to-service-requests>Cluster failing to service requests<a class=headerlink href=#cluster-failing-to-service-requests title="Permanent link">&para;</a></h2> <h3 id=symptoms>Symptoms<a class=headerlink href=#symptoms title="Permanent link">&para;</a></h3> <ul> <li>Execution of the <code>ceph</code> command hangs</li> <li>PersistentVolumes are not being created</li> <li>Large amount of slow requests are blocking</li> <li>Large amount of stuck requests are blocking</li> <li>One or more MONs are restarting periodically</li> </ul> <h3 id=investigation>Investigation<a class=headerlink href=#investigation title="Permanent link">&para;</a></h3> <p>Create a <a href=../ceph-toolbox/ >rook-ceph-tools pod</a> to investigate the current state of Ceph. Here is an example of what one might see. In this case the <code>ceph status</code> command would just hang so a CTRL-C needed to be sent.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-1-1>1</a></span>
<span class=normal><a href=#__codelineno-1-2>2</a></span>
<span class=normal><a href=#__codelineno-1-3>3</a></span>
<span class=normal><a href=#__codelineno-1-4>4</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-1-1 name=__codelineno-1-1></a><span class=go>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status</span>
<a id=__codelineno-1-2 name=__codelineno-1-2></a>
<a id=__codelineno-1-3 name=__codelineno-1-3></a><span class=go>ceph status</span>
<a id=__codelineno-1-4 name=__codelineno-1-4></a><span class=go>^CCluster connection interrupted or timed out</span>
</code></pre></div></td></tr></table></div> <p>Another indication is when one or more of the MON pods restart frequently. Note the 'mon107' that has only been up for 16 minutes in the following output.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-2-1>1</a></span>
<span class=normal><a href=#__codelineno-2-2>2</a></span>
<span class=normal><a href=#__codelineno-2-3>3</a></span>
<span class=normal><a href=#__codelineno-2-4>4</a></span>
<span class=normal><a href=#__codelineno-2-5>5</a></span>
<span class=normal><a href=#__codelineno-2-6>6</a></span>
<span class=normal><a href=#__codelineno-2-7>7</a></span>
<span class=normal><a href=#__codelineno-2-8>8</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-2-1 name=__codelineno-2-1></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span>get<span class=w> </span>all<span class=w> </span>-o<span class=w> </span>wide<span class=w> </span>--show-all
<a id=__codelineno-2-2 name=__codelineno-2-2></a><span class=go>NAME                                 READY     STATUS    RESTARTS   AGE       IP               NODE</span>
<a id=__codelineno-2-3 name=__codelineno-2-3></a><span class=go>po/rook-ceph-mgr0-2487684371-gzlbq   1/1       Running   0          17h       192.168.224.46   k8-host-0402</span>
<a id=__codelineno-2-4 name=__codelineno-2-4></a><span class=go>po/rook-ceph-mon107-p74rj            1/1       Running   0          16m       192.168.224.28   k8-host-0402</span>
<a id=__codelineno-2-5 name=__codelineno-2-5></a><span class=go>rook-ceph-mon1-56fgm                 1/1       Running   0          2d        192.168.91.135   k8-host-0404</span>
<a id=__codelineno-2-6 name=__codelineno-2-6></a><span class=go>rook-ceph-mon2-rlxcd                 1/1       Running   0          2d        192.168.123.33   k8-host-0403</span>
<a id=__codelineno-2-7 name=__codelineno-2-7></a><span class=go>rook-ceph-osd-bg2vj                  1/1       Running   0          2d        192.168.91.177   k8-host-0404</span>
<a id=__codelineno-2-8 name=__codelineno-2-8></a><span class=go>rook-ceph-osd-mwxdm                  1/1       Running   0          2d        192.168.123.31   k8-host-0403</span>
</code></pre></div></td></tr></table></div> <h3 id=solution>Solution<a class=headerlink href=#solution title="Permanent link">&para;</a></h3> <p>What is happening here is that the MON pods are restarting and one or more of the Ceph daemons are not getting configured with the proper cluster information. This is commonly the result of not specifying a value for <code>dataDirHostPath</code> in your Cluster CRD.</p> <p>The <code>dataDirHostPath</code> setting specifies a path on the local host for the Ceph daemons to store configuration and data. Setting this to a path like <code>/var/lib/rook</code>, reapplying your Cluster CRD and restarting all the Ceph daemons (MON, MGR, OSD, RGW) should solve this problem. After the Ceph daemons have been restarted, it is advisable to restart the <a href=../ceph-toolbox/ >rook-tools pod</a>.</p> <h2 id=monitors-are-the-only-pods-running>Monitors are the only pods running<a class=headerlink href=#monitors-are-the-only-pods-running title="Permanent link">&para;</a></h2> <h3 id=symptoms_1>Symptoms<a class=headerlink href=#symptoms_1 title="Permanent link">&para;</a></h3> <ul> <li>Rook operator is running</li> <li>Either a single mon starts or the mons start very slowly (at least several minutes apart)</li> <li>The crash-collector pods are crashing</li> <li>No mgr, osd, or other daemons are created except the CSI driver</li> </ul> <h3 id=investigation_1>Investigation<a class=headerlink href=#investigation_1 title="Permanent link">&para;</a></h3> <p>When the operator is starting a cluster, the operator will start one mon at a time and check that they are healthy before continuing to bring up all three mons. If the first mon is not detected healthy, the operator will continue to check until it is healthy. If the first mon fails to start, a second and then a third mon may attempt to start. However, they will never form quorum and the orchestration will be blocked from proceeding.</p> <p>The crash-collector pods will be blocked from starting until the mons have formed quorum the first time.</p> <p>There are several common causes for the mons failing to form quorum:</p> <ul> <li>The operator pod does not have network connectivity to the mon pod(s). The network may be configured incorrectly.</li> <li>One or more mon pods are in running state, but the operator log shows they are not able to form quorum</li> <li>A mon is using configuration from a previous installation. See the <a href=../../Storage-Configuration/ceph-teardown/#delete-the-data-on-hosts>cleanup guide</a> for cleaning the previous cluster.</li> <li>A firewall may be blocking the ports required for the Ceph mons to form quorum. Ensure ports 6789 and 3300 are enabled. See the <a href=https://docs.ceph.com/en/latest/rados/configuration/network-config-ref/ >Ceph networking guide</a> for more details.</li> <li>There may be MTU mismatch between different networking components. Some networks may be more susceptible to mismatch than others. If Kubernetes CNI or hosts enable jumbo frames (MTU 9000), Ceph will use large packets to maximize network bandwidth. If other parts of the networking chain don't support jumbo frames, this could result in lost or rejected packets unexpectedly.</li> </ul> <h4 id=operator-fails-to-connect-to-the-mon>Operator fails to connect to the mon<a class=headerlink href=#operator-fails-to-connect-to-the-mon title="Permanent link">&para;</a></h4> <p>First look at the logs of the operator to confirm if it is able to connect to the mons.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-3-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-3-1 name=__codelineno-3-1></a><span class=go>kubectl -n rook-ceph logs -l app=rook-ceph-operator</span>
</code></pre></div></td></tr></table></div> <p>Likely you will see an error similar to the following that the operator is timing out when connecting to the mon. The last command is <code>ceph mon_status</code>, followed by a timeout message five minutes later.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-4-1>1</a></span>
<span class=normal><a href=#__codelineno-4-2>2</a></span>
<span class=normal><a href=#__codelineno-4-3>3</a></span>
<span class=normal><a href=#__codelineno-4-4>4</a></span>
<span class=normal><a href=#__codelineno-4-5>5</a></span>
<span class=normal><a href=#__codelineno-4-6>6</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-4-1 name=__codelineno-4-1></a><span class=go>2018-01-21 21:47:32.375833 I | exec: Running command: ceph mon_status --cluster=rook --conf=/var/lib/rook/rook-ceph/rook.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/442263890</span>
<a id=__codelineno-4-2 name=__codelineno-4-2></a><span class=go>2018-01-21 21:52:35.370533 I | exec: 2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300</span>
<a id=__codelineno-4-3 name=__codelineno-4-3></a><span class=go>2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300</span>
<a id=__codelineno-4-4 name=__codelineno-4-4></a><span class=go>2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out</span>
<a id=__codelineno-4-5 name=__codelineno-4-5></a><span class=go>2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out</span>
<a id=__codelineno-4-6 name=__codelineno-4-6></a><span class=go>[errno 110] error connecting to the cluster</span>
</code></pre></div></td></tr></table></div> <p>The error would appear to be an authentication error, but it is misleading. The real issue is a timeout.</p> <h4 id=solution_1>Solution<a class=headerlink href=#solution_1 title="Permanent link">&para;</a></h4> <p>If you see the timeout in the operator log, verify if the mon pod is running (see the next section). If the mon pod is running, check the network connectivity between the operator pod and the mon pod. A common issue is that the CNI is not configured correctly.</p> <p>To verify the network connectivity:</p> <ul> <li>Get the endpoint for a mon</li> <li>Curl the mon from the operator pod</li> </ul> <p>For example, this command will curl the first mon from the operator:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-5-1>1</a></span>
<span class=normal><a href=#__codelineno-5-2>2</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-5-1 name=__codelineno-5-1></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span><span class=nb>exec</span><span class=w> </span>deploy/rook-ceph-operator<span class=w> </span>--<span class=w> </span>curl<span class=w> </span><span class=k>$(</span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span>get<span class=w> </span>svc<span class=w> </span>-l<span class=w> </span><span class=nv>app</span><span class=o>=</span>rook-ceph-mon<span class=w> </span>-o<span class=w> </span><span class=nv>jsonpath</span><span class=o>=</span><span class=s1>&#39;{.items[0].spec.clusterIP}&#39;</span><span class=k>)</span>:3300<span class=w> </span><span class=m>2</span>&gt;/dev/null
<a id=__codelineno-5-2 name=__codelineno-5-2></a><span class=go>ceph v2</span>
</code></pre></div></td></tr></table></div> <p>If "ceph v2" is printed to the console, the connection was successful. If the command does not respond or otherwise fails, the network connection cannot be established.</p> <h4 id=failing-mon-pod>Failing mon pod<a class=headerlink href=#failing-mon-pod title="Permanent link">&para;</a></h4> <p>Second we need to verify if the mon pod started successfully.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-6-1>1</a></span>
<span class=normal><a href=#__codelineno-6-2>2</a></span>
<span class=normal><a href=#__codelineno-6-3>3</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-6-1 name=__codelineno-6-1></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span>get<span class=w> </span>pod<span class=w> </span>-l<span class=w> </span><span class=nv>app</span><span class=o>=</span>rook-ceph-mon
<a id=__codelineno-6-2 name=__codelineno-6-2></a><span class=go>NAME                                READY     STATUS               RESTARTS   AGE</span>
<a id=__codelineno-6-3 name=__codelineno-6-3></a><span class=go>rook-ceph-mon-a-69fb9c78cd-58szd    1/1       CrashLoopBackOff     2          47s</span>
</code></pre></div></td></tr></table></div> <p>If the mon pod is failing as in this example, you will need to look at the mon pod status or logs to determine the cause. If the pod is in a crash loop backoff state, you should see the reason by describing the pod.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-7-1>1</a></span>
<span class=normal><a href=#__codelineno-7-2>2</a></span>
<span class=normal><a href=#__codelineno-7-3>3</a></span>
<span class=normal><a href=#__codelineno-7-4>4</a></span>
<span class=normal><a href=#__codelineno-7-5>5</a></span>
<span class=normal><a href=#__codelineno-7-6>6</a></span>
<span class=normal><a href=#__codelineno-7-7>7</a></span>
<span class=normal><a href=#__codelineno-7-8>8</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-7-1 name=__codelineno-7-1></a><span class=gp># </span>The<span class=w> </span>pod<span class=w> </span>shows<span class=w> </span>a<span class=w> </span>termination<span class=w> </span>status<span class=w> </span>that<span class=w> </span>the<span class=w> </span>keyring<span class=w> </span>does<span class=w> </span>not<span class=w> </span>match<span class=w> </span>the<span class=w> </span>existing<span class=w> </span>keyring
<a id=__codelineno-7-2 name=__codelineno-7-2></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span>describe<span class=w> </span>pod<span class=w> </span>-l<span class=w> </span><span class=nv>mon</span><span class=o>=</span>rook-ceph-mon0
<a id=__codelineno-7-3 name=__codelineno-7-3></a><span class=go>...</span>
<a id=__codelineno-7-4 name=__codelineno-7-4></a><span class=go>    Last State:    Terminated</span>
<a id=__codelineno-7-5 name=__codelineno-7-5></a><span class=go>      Reason:    Error</span>
<a id=__codelineno-7-6 name=__codelineno-7-6></a><span class=go>      Message:    The keyring does not match the existing keyring in /var/lib/rook/rook-ceph-mon0/data/keyring.</span>
<a id=__codelineno-7-7 name=__codelineno-7-7></a><span class=go>                    You may need to delete the contents of dataDirHostPath on the host from a previous deployment.</span>
<a id=__codelineno-7-8 name=__codelineno-7-8></a><span class=go>...</span>
</code></pre></div></td></tr></table></div> <p>See the solution in the next section regarding cleaning up the <code>dataDirHostPath</code> on the nodes.</p> <h4 id=solution_2>Solution<a class=headerlink href=#solution_2 title="Permanent link">&para;</a></h4> <p>This is a common problem reinitializing the Rook cluster when the local directory used for persistence has <strong>not</strong> been purged. This directory is the <code>dataDirHostPath</code> setting in the cluster CRD and is typically set to <code>/var/lib/rook</code>. To fix the issue you will need to delete all components of Rook and then delete the contents of <code>/var/lib/rook</code> (or the directory specified by <code>dataDirHostPath</code>) on each of the hosts in the cluster. Then when the cluster CRD is applied to start a new cluster, the rook-operator should start all the pods as expected.</p> <div class="admonition caution"> <p class=admonition-title>Caution</p> <p><strong>Deleting the <code>dataDirHostPath</code> folder is destructive to the storage. Only delete the folder if you are trying to permanently purge the Rook cluster.</strong></p> </div> <p>See the <a href=../../Storage-Configuration/ceph-teardown/ >Cleanup Guide</a> for more details.</p> <h2 id=pvcs-stay-in-pending-state>PVCs stay in pending state<a class=headerlink href=#pvcs-stay-in-pending-state title="Permanent link">&para;</a></h2> <h3 id=symptoms_2>Symptoms<a class=headerlink href=#symptoms_2 title="Permanent link">&para;</a></h3> <ul> <li>When you create a PVC based on a rook storage class, it stays pending indefinitely</li> </ul> <p>For the Wordpress example, you might see two PVCs in pending state.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-8-1>1</a></span>
<span class=normal><a href=#__codelineno-8-2>2</a></span>
<span class=normal><a href=#__codelineno-8-3>3</a></span>
<span class=normal><a href=#__codelineno-8-4>4</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-8-1 name=__codelineno-8-1></a><span class=gp>$ </span>kubectl<span class=w> </span>get<span class=w> </span>pvc
<a id=__codelineno-8-2 name=__codelineno-8-2></a><span class=go>NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE</span>
<a id=__codelineno-8-3 name=__codelineno-8-3></a><span class=go>mysql-pv-claim   Pending                                      rook-ceph-block   8s</span>
<a id=__codelineno-8-4 name=__codelineno-8-4></a><span class=go>wp-pv-claim      Pending                                      rook-ceph-block   16s</span>
</code></pre></div></td></tr></table></div> <h3 id=investigation_2>Investigation<a class=headerlink href=#investigation_2 title="Permanent link">&para;</a></h3> <p>There are two common causes for the PVCs staying in pending state:</p> <ol> <li>There are no OSDs in the cluster</li> <li>The CSI provisioner pod is not running or is not responding to the request to provision the storage</li> </ol> <h4 id=confirm-if-there-are-osds>Confirm if there are OSDs<a class=headerlink href=#confirm-if-there-are-osds title="Permanent link">&para;</a></h4> <p>To confirm if you have OSDs in your cluster, connect to the <a href=../ceph-toolbox/ >Rook Toolbox</a> and run the <code>ceph status</code> command. You should see that you have at least one OSD <code>up</code> and <code>in</code>. The minimum number of OSDs required depends on the <code>replicated.size</code> setting in the pool created for the storage class. In a "test" cluster, only one OSD is required (see <code>storageclass-test.yaml</code>). In the production storage class example (<code>storageclass.yaml</code>), three OSDs would be required.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-9-1>1</a></span>
<span class=normal><a href=#__codelineno-9-2>2</a></span>
<span class=normal><a href=#__codelineno-9-3>3</a></span>
<span class=normal><a href=#__codelineno-9-4>4</a></span>
<span class=normal><a href=#__codelineno-9-5>5</a></span>
<span class=normal><a href=#__codelineno-9-6>6</a></span>
<span class=normal><a href=#__codelineno-9-7>7</a></span>
<span class=normal><a href=#__codelineno-9-8>8</a></span>
<span class=normal><a href=#__codelineno-9-9>9</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-9-1 name=__codelineno-9-1></a><span class=gp>$ </span>ceph<span class=w> </span>status
<a id=__codelineno-9-2 name=__codelineno-9-2></a><span class=go>  cluster:</span>
<a id=__codelineno-9-3 name=__codelineno-9-3></a><span class=go>    id:     a0452c76-30d9-4c1a-a948-5d8405f19a7c</span>
<a id=__codelineno-9-4 name=__codelineno-9-4></a><span class=go>    health: HEALTH_OK</span>
<a id=__codelineno-9-5 name=__codelineno-9-5></a>
<a id=__codelineno-9-6 name=__codelineno-9-6></a><span class=go>  services:</span>
<a id=__codelineno-9-7 name=__codelineno-9-7></a><span class=go>    mon: 3 daemons, quorum a,b,c (age 11m)</span>
<a id=__codelineno-9-8 name=__codelineno-9-8></a><span class=go>    mgr: a(active, since 10m)</span>
<a id=__codelineno-9-9 name=__codelineno-9-9></a><span class=go>    osd: 1 osds: 1 up (since 46s), 1 in (since 109m)</span>
</code></pre></div></td></tr></table></div> <h4 id=osd-prepare-logs>OSD Prepare Logs<a class=headerlink href=#osd-prepare-logs title="Permanent link">&para;</a></h4> <p>If you don't see the expected number of OSDs, let's investigate why they weren't created. On each node where Rook looks for OSDs to configure, you will see an "osd prepare" pod.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-10-1>1</a></span>
<span class=normal><a href=#__codelineno-10-2>2</a></span>
<span class=normal><a href=#__codelineno-10-3>3</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-10-1 name=__codelineno-10-1></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span>get<span class=w> </span>pod<span class=w> </span>-l<span class=w> </span><span class=nv>app</span><span class=o>=</span>rook-ceph-osd-prepare
<a id=__codelineno-10-2 name=__codelineno-10-2></a><span class=go>NAME                                 ...  READY   STATUS      RESTARTS   AGE</span>
<a id=__codelineno-10-3 name=__codelineno-10-3></a><span class=go>rook-ceph-osd-prepare-minikube-9twvk   0/2     Completed   0          30m</span>
</code></pre></div></td></tr></table></div> <p>See the section on <a href=#osd-pods-are-not-created-on-my-devices>why OSDs are not getting created</a> to investigate the logs.</p> <h4 id=csi-driver>CSI Driver<a class=headerlink href=#csi-driver title="Permanent link">&para;</a></h4> <p>The CSI driver may not be responding to the requests. Look in the logs of the CSI provisioner pod to see if there are any errors during the provisioning.</p> <p>There are two provisioner pods:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-11-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-11-1 name=__codelineno-11-1></a><span class=go>kubectl -n rook-ceph get pod -l app=csi-rbdplugin-provisioner</span>
</code></pre></div></td></tr></table></div> <p>Get the logs of each of the pods. One of them should be the "leader" and be responding to requests.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-12-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-12-1 name=__codelineno-12-1></a><span class=go>kubectl -n rook-ceph logs csi-cephfsplugin-provisioner-d77bb49c6-q9hwq csi-provisioner</span>
</code></pre></div></td></tr></table></div> <p>See also the <a href=../ceph-csi-common-issues/ >CSI Troubleshooting Guide</a>.</p> <h4 id=operator-unresponsiveness>Operator unresponsiveness<a class=headerlink href=#operator-unresponsiveness title="Permanent link">&para;</a></h4> <p>Lastly, if you have OSDs <code>up</code> and <code>in</code>, the next step is to confirm the operator is responding to the requests. Look in the Operator pod logs around the time when the PVC was created to confirm if the request is being raised. If the operator does not show requests to provision the block image, the operator may be stuck on some other operation. In this case, restart the operator pod to get things going again.</p> <h3 id=solution_3>Solution<a class=headerlink href=#solution_3 title="Permanent link">&para;</a></h3> <p>If the "osd prepare" logs didn't give you enough clues about why the OSDs were not being created, please review your <a href=../../CRDs/Cluster/ceph-cluster-crd/#storage-selection-settings>cluster.yaml</a> configuration. The common misconfigurations include:</p> <ul> <li>If <code>useAllDevices: true</code>, Rook expects to find local devices attached to the nodes. If no devices are found, no OSDs will be created.</li> <li>If <code>useAllDevices: false</code>, OSDs will only be created if <code>deviceFilter</code> is specified.</li> <li>Only local devices attached to the nodes will be configurable by Rook. In other words, the devices must show up under <code>/dev</code>.<ul> <li>The devices must not have any partitions or filesystems on them. Rook will only configure raw devices. Partitions are not yet supported.</li> </ul> </li> </ul> <h2 id=osd-pods-are-failing-to-start>OSD pods are failing to start<a class=headerlink href=#osd-pods-are-failing-to-start title="Permanent link">&para;</a></h2> <h3 id=symptoms_3>Symptoms<a class=headerlink href=#symptoms_3 title="Permanent link">&para;</a></h3> <ul> <li>OSD pods are failing to start</li> <li>You have started a cluster after tearing down another cluster</li> </ul> <h3 id=investigation_3>Investigation<a class=headerlink href=#investigation_3 title="Permanent link">&para;</a></h3> <p>When an OSD starts, the device or directory will be configured for consumption. If there is an error with the configuration, the pod will crash and you will see the CrashLoopBackoff status for the pod. Look in the osd pod logs for an indication of the failure.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-13-1>1</a></span>
<span class=normal><a href=#__codelineno-13-2>2</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-13-1 name=__codelineno-13-1></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span>logs<span class=w> </span>rook-ceph-osd-fl8fs
<a id=__codelineno-13-2 name=__codelineno-13-2></a><span class=go>...</span>
</code></pre></div></td></tr></table></div> <p>One common case for failure is that you have re-deployed a test cluster and some state may remain from a previous deployment. If your cluster is larger than a few nodes, you may get lucky enough that the monitors were able to start and form quorum. However, now the OSDs pods may fail to start due to the old state. Looking at the OSD pod logs you will see an error about the file already existing.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-14-1>1</a></span>
<span class=normal><a href=#__codelineno-14-2>2</a></span>
<span class=normal><a href=#__codelineno-14-3>3</a></span>
<span class=normal><a href=#__codelineno-14-4>4</a></span>
<span class=normal><a href=#__codelineno-14-5>5</a></span>
<span class=normal><a href=#__codelineno-14-6>6</a></span>
<span class=normal><a href=#__codelineno-14-7>7</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-14-1 name=__codelineno-14-1></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span>logs<span class=w> </span>rook-ceph-osd-fl8fs
<a id=__codelineno-14-2 name=__codelineno-14-2></a><span class=go>...</span>
<a id=__codelineno-14-3 name=__codelineno-14-3></a><span class=go>2017-10-31 20:13:11.187106 I | mkfs-osd0: 2017-10-31 20:13:11.186992 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _read_fsid unparsable uuid</span>
<a id=__codelineno-14-4 name=__codelineno-14-4></a><span class=go>2017-10-31 20:13:11.187208 I | mkfs-osd0: 2017-10-31 20:13:11.187026 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _setup_block_symlink_or_file failed to create block symlink to /dev/disk/by-partuuid/651153ba-2dfc-4231-ba06-94759e5ba273: (17) File exists</span>
<a id=__codelineno-14-5 name=__codelineno-14-5></a><span class=go>2017-10-31 20:13:11.187233 I | mkfs-osd0: 2017-10-31 20:13:11.187038 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) mkfs failed, (17) File exists</span>
<a id=__codelineno-14-6 name=__codelineno-14-6></a><span class=go>2017-10-31 20:13:11.187254 I | mkfs-osd0: 2017-10-31 20:13:11.187042 7f0059d62e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (17) File exists</span>
<a id=__codelineno-14-7 name=__codelineno-14-7></a><span class=go>2017-10-31 20:13:11.187275 I | mkfs-osd0: 2017-10-31 20:13:11.187121 7f0059d62e00 -1  ** ERROR: error creating empty object store in /var/lib/rook/osd0: (17) File exists</span>
</code></pre></div></td></tr></table></div> <h3 id=solution_4>Solution<a class=headerlink href=#solution_4 title="Permanent link">&para;</a></h3> <p>If the error is from the file that already exists, this is a common problem reinitializing the Rook cluster when the local directory used for persistence has <strong>not</strong> been purged. This directory is the <code>dataDirHostPath</code> setting in the cluster CRD and is typically set to <code>/var/lib/rook</code>. To fix the issue you will need to delete all components of Rook and then delete the contents of <code>/var/lib/rook</code> (or the directory specified by <code>dataDirHostPath</code>) on each of the hosts in the cluster. Then when the cluster CRD is applied to start a new cluster, the rook-operator should start all the pods as expected.</p> <h2 id=osd-pods-are-not-created-on-my-devices>OSD pods are not created on my devices<a class=headerlink href=#osd-pods-are-not-created-on-my-devices title="Permanent link">&para;</a></h2> <h3 id=symptoms_4>Symptoms<a class=headerlink href=#symptoms_4 title="Permanent link">&para;</a></h3> <ul> <li>No OSD pods are started in the cluster</li> <li>Devices are not configured with OSDs even though specified in the Cluster CRD</li> <li>One OSD pod is started on each node instead of multiple pods for each device</li> </ul> <h3 id=investigation_4>Investigation<a class=headerlink href=#investigation_4 title="Permanent link">&para;</a></h3> <p>First, ensure that you have specified the devices correctly in the CRD. The <a href=../../CRDs/Cluster/ceph-cluster-crd/#storage-selection-settings>Cluster CRD</a> has several ways to specify the devices that are to be consumed by the Rook storage:</p> <ul> <li><code>useAllDevices: true</code>: Rook will consume all devices it determines to be available</li> <li><code>deviceFilter</code>: Consume all devices that match this regular expression</li> <li><code>devices</code>: Explicit list of device names on each node to consume</li> </ul> <p>Second, if Rook determines that a device is not available (has existing partitions or a formatted filesystem), Rook will skip consuming the devices. If Rook is not starting OSDs on the devices you expect, Rook may have skipped it for this reason. To see if a device was skipped, view the OSD preparation log on the node where the device was skipped. Note that it is completely normal and expected for OSD prepare pod to be in the <code>completed</code> state. After the job is complete, Rook leaves the pod around in case the logs need to be investigated.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-15-1>1</a></span>
<span class=normal><a href=#__codelineno-15-2>2</a></span>
<span class=normal><a href=#__codelineno-15-3>3</a></span>
<span class=normal><a href=#__codelineno-15-4>4</a></span>
<span class=normal><a href=#__codelineno-15-5>5</a></span>
<span class=normal><a href=#__codelineno-15-6>6</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-15-1 name=__codelineno-15-1></a><span class=gp># </span>Get<span class=w> </span>the<span class=w> </span>prepare<span class=w> </span>pods<span class=w> </span><span class=k>in</span><span class=w> </span>the<span class=w> </span>cluster
<a id=__codelineno-15-2 name=__codelineno-15-2></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span>get<span class=w> </span>pod<span class=w> </span>-l<span class=w> </span><span class=nv>app</span><span class=o>=</span>rook-ceph-osd-prepare
<a id=__codelineno-15-3 name=__codelineno-15-3></a><span class=go>NAME                                   READY     STATUS      RESTARTS   AGE</span>
<a id=__codelineno-15-4 name=__codelineno-15-4></a><span class=go>rook-ceph-osd-prepare-node1-fvmrp      0/1       Completed   0          18m</span>
<a id=__codelineno-15-5 name=__codelineno-15-5></a><span class=go>rook-ceph-osd-prepare-node2-w9xv9      0/1       Completed   0          22m</span>
<a id=__codelineno-15-6 name=__codelineno-15-6></a><span class=go>rook-ceph-osd-prepare-node3-7rgnv      0/1       Completed   0          22m</span>
</code></pre></div></td></tr></table></div> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-16-1>1</a></span>
<span class=normal><a href=#__codelineno-16-2>2</a></span>
<span class=normal><a href=#__codelineno-16-3>3</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-16-1 name=__codelineno-16-1></a><span class=gp># </span>view<span class=w> </span>the<span class=w> </span>logs<span class=w> </span><span class=k>for</span><span class=w> </span>the<span class=w> </span>node<span class=w> </span>of<span class=w> </span>interest<span class=w> </span><span class=k>in</span><span class=w> </span>the<span class=w> </span><span class=s2>&quot;provision&quot;</span><span class=w> </span>container
<a id=__codelineno-16-2 name=__codelineno-16-2></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span>logs<span class=w> </span>rook-ceph-osd-prepare-node1-fvmrp<span class=w> </span>provision
<a id=__codelineno-16-3 name=__codelineno-16-3></a><span class=go>[...]</span>
</code></pre></div></td></tr></table></div> <p>Here are some key lines to look for in the log:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-17-1> 1</a></span>
<span class=normal><a href=#__codelineno-17-2> 2</a></span>
<span class=normal><a href=#__codelineno-17-3> 3</a></span>
<span class=normal><a href=#__codelineno-17-4> 4</a></span>
<span class=normal><a href=#__codelineno-17-5> 5</a></span>
<span class=normal><a href=#__codelineno-17-6> 6</a></span>
<span class=normal><a href=#__codelineno-17-7> 7</a></span>
<span class=normal><a href=#__codelineno-17-8> 8</a></span>
<span class=normal><a href=#__codelineno-17-9> 9</a></span>
<span class=normal><a href=#__codelineno-17-10>10</a></span>
<span class=normal><a href=#__codelineno-17-11>11</a></span>
<span class=normal><a href=#__codelineno-17-12>12</a></span>
<span class=normal><a href=#__codelineno-17-13>13</a></span>
<span class=normal><a href=#__codelineno-17-14>14</a></span>
<span class=normal><a href=#__codelineno-17-15>15</a></span>
<span class=normal><a href=#__codelineno-17-16>16</a></span>
<span class=normal><a href=#__codelineno-17-17>17</a></span>
<span class=normal><a href=#__codelineno-17-18>18</a></span>
<span class=normal><a href=#__codelineno-17-19>19</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-17-1 name=__codelineno-17-1></a><span class=gp># </span>A<span class=w> </span>device<span class=w> </span>will<span class=w> </span>be<span class=w> </span>skipped<span class=w> </span><span class=k>if</span><span class=w> </span>Rook<span class=w> </span>sees<span class=w> </span>it<span class=w> </span>has<span class=w> </span>partitions<span class=w> </span>or<span class=w> </span>a<span class=w> </span>filesystem
<a id=__codelineno-17-2 name=__codelineno-17-2></a><span class=go>2019-05-30 19:02:57.353171 W | cephosd: skipping device sda that is in use</span>
<a id=__codelineno-17-3 name=__codelineno-17-3></a><span class=go>2019-05-30 19:02:57.452168 W | skipping device &quot;sdb5&quot;: [&quot;Used by ceph-disk&quot;]</span>
<a id=__codelineno-17-4 name=__codelineno-17-4></a>
<a id=__codelineno-17-5 name=__codelineno-17-5></a><span class=gp># </span>Other<span class=w> </span>messages<span class=w> </span>about<span class=w> </span>a<span class=w> </span>disk<span class=w> </span>being<span class=w> </span>unusable<span class=w> </span>by<span class=w> </span>ceph<span class=w> </span>include:
<a id=__codelineno-17-6 name=__codelineno-17-6></a><span class=go>Insufficient space (&lt;5GB) on vgs</span>
<a id=__codelineno-17-7 name=__codelineno-17-7></a><span class=go>Insufficient space (&lt;5GB)</span>
<a id=__codelineno-17-8 name=__codelineno-17-8></a><span class=go>LVM detected</span>
<a id=__codelineno-17-9 name=__codelineno-17-9></a><span class=go>Has BlueStore device label</span>
<a id=__codelineno-17-10 name=__codelineno-17-10></a><span class=go>locked</span>
<a id=__codelineno-17-11 name=__codelineno-17-11></a><span class=go>read-only</span>
<a id=__codelineno-17-12 name=__codelineno-17-12></a>
<a id=__codelineno-17-13 name=__codelineno-17-13></a><span class=gp># </span>A<span class=w> </span>device<span class=w> </span>is<span class=w> </span>going<span class=w> </span>to<span class=w> </span>be<span class=w> </span>configured
<a id=__codelineno-17-14 name=__codelineno-17-14></a><span class=go>2019-05-30 19:02:57.535598 I | cephosd: device sdc to be configured by ceph-volume</span>
<a id=__codelineno-17-15 name=__codelineno-17-15></a>
<a id=__codelineno-17-16 name=__codelineno-17-16></a><span class=gp># </span>For<span class=w> </span>each<span class=w> </span>device<span class=w> </span>configured<span class=w> </span>you<span class=w> </span>will<span class=w> </span>see<span class=w> </span>a<span class=w> </span>report<span class=w> </span>printed<span class=w> </span>to<span class=w> </span>the<span class=w> </span>log
<a id=__codelineno-17-17 name=__codelineno-17-17></a><span class=go>2019-05-30 19:02:59.844642 I |   Type            Path                                                    LV Size         % of device</span>
<a id=__codelineno-17-18 name=__codelineno-17-18></a><span class=go>2019-05-30 19:02:59.844651 I | ----------------------------------------------------------------------------------------------------</span>
<a id=__codelineno-17-19 name=__codelineno-17-19></a><span class=go>2019-05-30 19:02:59.844677 I |   [data]          /dev/sdc                                                7.00 GB         100%</span>
</code></pre></div></td></tr></table></div> <h3 id=solution_5>Solution<a class=headerlink href=#solution_5 title="Permanent link">&para;</a></h3> <p>Either update the CR with the correct settings, or clean the partitions or filesystem from your devices. To clean devices from a previous install see the <a href=../../Storage-Configuration/ceph-teardown/#zapping-devices>cleanup guide</a>.</p> <p>After the settings are updated or the devices are cleaned, trigger the operator to analyze the devices again by restarting the operator. Each time the operator starts, it will ensure all the desired devices are configured. The operator does automatically deploy OSDs in most scenarios, but an operator restart will cover any scenarios that the operator doesn't detect automatically.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-18-1>1</a></span>
<span class=normal><a href=#__codelineno-18-2>2</a></span>
<span class=normal><a href=#__codelineno-18-3>3</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-18-1 name=__codelineno-18-1></a><span class=gp># </span>Restart<span class=w> </span>the<span class=w> </span>operator<span class=w> </span>to<span class=w> </span>ensure<span class=w> </span>devices<span class=w> </span>are<span class=w> </span>configured.<span class=w> </span>A<span class=w> </span>new<span class=w> </span>pod<span class=w> </span>will<span class=w> </span>automatically<span class=w> </span>be<span class=w> </span>started<span class=w> </span>when<span class=w> </span>the<span class=w> </span>current<span class=w> </span>operator<span class=w> </span>pod<span class=w> </span>is<span class=w> </span>deleted.
<a id=__codelineno-18-2 name=__codelineno-18-2></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span>delete<span class=w> </span>pod<span class=w> </span>-l<span class=w> </span><span class=nv>app</span><span class=o>=</span>rook-ceph-operator
<a id=__codelineno-18-3 name=__codelineno-18-3></a><span class=go>[...]</span>
</code></pre></div></td></tr></table></div> <h2 id=node-hangs-after-reboot>Node hangs after reboot<a class=headerlink href=#node-hangs-after-reboot title="Permanent link">&para;</a></h2> <p>This issue is fixed in Rook v1.3 or later.</p> <h3 id=symptoms_5>Symptoms<a class=headerlink href=#symptoms_5 title="Permanent link">&para;</a></h3> <ul> <li>After issuing a <code>reboot</code> command, node never returned online</li> <li>Only a power cycle helps</li> </ul> <h3 id=investigation_5>Investigation<a class=headerlink href=#investigation_5 title="Permanent link">&para;</a></h3> <p>On a node running a pod with a Ceph persistent volume</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-19-1>1</a></span>
<span class=normal><a href=#__codelineno-19-2>2</a></span>
<span class=normal><a href=#__codelineno-19-3>3</a></span>
<span class=normal><a href=#__codelineno-19-4>4</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-19-1 name=__codelineno-19-1></a><span class=go>mount | grep rbd</span>
<a id=__codelineno-19-2 name=__codelineno-19-2></a><span class=gp># </span>_netdev<span class=w> </span>mount<span class=w> </span>option<span class=w> </span>is<span class=w> </span>absent,<span class=w> </span>also<span class=w> </span>occurs<span class=w> </span><span class=k>for</span><span class=w> </span>cephfs
<a id=__codelineno-19-3 name=__codelineno-19-3></a><span class=gp># </span>OS<span class=w> </span>is<span class=w> </span>not<span class=w> </span>aware<span class=w> </span>PV<span class=w> </span>is<span class=w> </span>mounted<span class=w> </span>over<span class=w> </span>network
<a id=__codelineno-19-4 name=__codelineno-19-4></a><span class=go>/dev/rbdx on ... (rw,relatime, ..., noquota)</span>
</code></pre></div></td></tr></table></div> <p>When the reboot command is issued, network interfaces are terminated before disks are unmounted. This results in the node hanging as repeated attempts to unmount Ceph persistent volumes fail with the following error:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-20-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-20-1 name=__codelineno-20-1></a><span class=go>libceph: connect [monitor-ip]:6789 error -101</span>
</code></pre></div></td></tr></table></div> <h3 id=solution_6>Solution<a class=headerlink href=#solution_6 title="Permanent link">&para;</a></h3> <p>The node needs to be <a href=https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/ >drained</a> before reboot. After the successful drain, the node can be rebooted as usual.</p> <p>Because <code>kubectl drain</code> command automatically marks the node as unschedulable (<code>kubectl cordon</code> effect), the node needs to be uncordoned once it's back online.</p> <p>Drain the node:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-21-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-21-1 name=__codelineno-21-1></a><span class=go>kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data</span>
</code></pre></div></td></tr></table></div> <p>Uncordon the node:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-22-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-22-1 name=__codelineno-22-1></a><span class=go>kubectl uncordon &lt;node-name&gt;</span>
</code></pre></div></td></tr></table></div> <h2 id=using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47>Using multiple shared filesystem (CephFS) is attempted on a kernel version older than 4.7<a class=headerlink href=#using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47 title="Permanent link">&para;</a></h2> <h3 id=symptoms_6>Symptoms<a class=headerlink href=#symptoms_6 title="Permanent link">&para;</a></h3> <ul> <li>More than one shared filesystem (CephFS) has been created in the cluster</li> <li>A pod attempts to mount any other shared filesystem besides the <strong>first</strong> one that was created</li> <li>The pod incorrectly gets the first filesystem mounted instead of the intended filesystem</li> </ul> <h3 id=solution_7>Solution<a class=headerlink href=#solution_7 title="Permanent link">&para;</a></h3> <p>The only solution to this problem is to upgrade your kernel to <code>4.7</code> or higher. This is due to a mount flag added in the kernel version <code>4.7</code> which allows to chose the filesystem by name.</p> <p>For additional info on the kernel version requirement for multiple shared filesystems (CephFS), see <a href=../../Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage/#kernel-version-requirement>Filesystem - Kernel version requirement</a>.</p> <h2 id=set-debug-log-level-for-all-ceph-daemons>Set debug log level for all Ceph daemons<a class=headerlink href=#set-debug-log-level-for-all-ceph-daemons title="Permanent link">&para;</a></h2> <p>You can set a given log level and apply it to all the Ceph daemons at the same time. For this, make sure the toolbox pod is running, then determine the level you want (between 0 and 20). You can find the list of all subsystems and their default values in <a href=https://docs.ceph.com/en/latest/rados/troubleshooting/log-and-debug/#ceph-subsystems>Ceph logging and debug official guide</a>. Be careful when increasing the level as it will produce very verbose logs.</p> <p>Assuming you want a log level of 1, you will run:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-23-1>1</a></span>
<span class=normal><a href=#__codelineno-23-2>2</a></span>
<span class=normal><a href=#__codelineno-23-3>3</a></span>
<span class=normal><a href=#__codelineno-23-4>4</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-23-1 name=__codelineno-23-1></a><span class=gp>$ </span>kubectl<span class=w> </span>-n<span class=w> </span>rook-ceph<span class=w> </span><span class=nb>exec</span><span class=w> </span>deploy/rook-ceph-tools<span class=w> </span>--<span class=w> </span>set-ceph-debug-level<span class=w> </span><span class=m>1</span>
<a id=__codelineno-23-2 name=__codelineno-23-2></a><span class=go>ceph config set global debug_context 1</span>
<a id=__codelineno-23-3 name=__codelineno-23-3></a><span class=go>ceph config set global debug_lockdep 1</span>
<a id=__codelineno-23-4 name=__codelineno-23-4></a><span class=go>[...]</span>
</code></pre></div></td></tr></table></div> <p>Once you are done debugging, you can revert all the debug flag to their default value by running the following:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-24-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-24-1 name=__codelineno-24-1></a><span class=go>kubectl -n rook-ceph exec deploy/rook-ceph-tools -- set-ceph-debug-level default</span>
</code></pre></div></td></tr></table></div> <h2 id=activate-log-to-file-for-a-particular-ceph-daemon>Activate log to file for a particular Ceph daemon<a class=headerlink href=#activate-log-to-file-for-a-particular-ceph-daemon title="Permanent link">&para;</a></h2> <p>They are cases where looking at Kubernetes logs is not enough for diverse reasons, but just to name a few:</p> <ul> <li>not everyone is familiar for Kubernetes logging and expects to find logs in traditional directories</li> <li>logs get eaten (buffer limit from the log engine) and thus not requestable from Kubernetes</li> </ul> <p>So for each daemon, <code>dataDirHostPath</code> is used to store logs, if logging is activated. Rook will bindmount <code>dataDirHostPath</code> for every pod. Let's say you want to enable logging for <code>mon.a</code>, but only for this daemon. Using the toolbox or from inside the operator run:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-25-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-25-1 name=__codelineno-25-1></a><span class=go>ceph config set mon.a log_to_file true</span>
</code></pre></div></td></tr></table></div> <p>This will activate logging on the filesystem, you will be able to find logs in <code>dataDirHostPath/$NAMESPACE/log</code>, so typically this would mean <code>/var/lib/rook/rook-ceph/log</code>. You don't need to restart the pod, the effect will be immediate.</p> <p>To disable the logging on file, simply set <code>log_to_file</code> to <code>false</code>.</p> <h2 id=a-worker-node-using-rbd-devices-hangs-up>A worker node using RBD devices hangs up<a class=headerlink href=#a-worker-node-using-rbd-devices-hangs-up title="Permanent link">&para;</a></h2> <h3 id=symptoms_7>Symptoms<a class=headerlink href=#symptoms_7 title="Permanent link">&para;</a></h3> <ul> <li>There is no progress on I/O from/to one of RBD devices (<code>/dev/rbd*</code> or <code>/dev/nbd*</code>).</li> <li>After that, the whole worker node hangs up.</li> </ul> <h3 id=investigation_6>Investigation<a class=headerlink href=#investigation_6 title="Permanent link">&para;</a></h3> <p>This happens when the following conditions are satisfied.</p> <ul> <li>The problematic RBD device and the corresponding OSDs are co-located.</li> <li>There is an XFS filesystem on top of this device.</li> </ul> <p>In addition, when this problem happens, you can see the following messages in <code>dmesg</code>.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-26-1>1</a></span>
<span class=normal><a href=#__codelineno-26-2>2</a></span>
<span class=normal><a href=#__codelineno-26-3>3</a></span>
<span class=normal><a href=#__codelineno-26-4>4</a></span>
<span class=normal><a href=#__codelineno-26-5>5</a></span>
<span class=normal><a href=#__codelineno-26-6>6</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-26-1 name=__codelineno-26-1></a><span class=gp>$ </span>dmesg
<a id=__codelineno-26-2 name=__codelineno-26-2></a><span class=go>...</span>
<a id=__codelineno-26-3 name=__codelineno-26-3></a><span class=go>[51717.039319] INFO: task kworker/2:1:5938 blocked for more than 120 seconds.</span>
<a id=__codelineno-26-4 name=__codelineno-26-4></a><span class=go>[51717.039361]       Not tainted 4.15.0-72-generic #81-Ubuntu</span>
<a id=__codelineno-26-5 name=__codelineno-26-5></a><span class=go>[51717.039388] &quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot; disables this message.</span>
<a id=__codelineno-26-6 name=__codelineno-26-6></a><span class=go>...</span>
</code></pre></div></td></tr></table></div> <p>It's so-called <code>hung_task</code> problem and means that there is a deadlock in the kernel. For more detail, please refer to <a href=https://github.com/rook/rook/issues/3132#issuecomment-580508760>the corresponding issue comment</a>.</p> <h3 id=solution_8>Solution<a class=headerlink href=#solution_8 title="Permanent link">&para;</a></h3> <p>This problem will be solve by the following two fixes.</p> <ul> <li>Linux kernel: A minor feature that is introduced by <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=8d19f1c8e1937baf74e1962aae9f90fa3aeab463">this commit</a>. It will be included in Linux v5.6.</li> <li>Ceph: A fix that uses the above-mentioned kernel's feature. The Ceph community will probably discuss this fix after releasing Linux v5.6.</li> </ul> <p>You can bypass this problem by using ext4 or any other filesystems rather than XFS. Filesystem type can be specified with <code>csi.storage.k8s.io/fstype</code> in StorageClass resource.</p> <h2 id=too-few-pgs-per-osd-warning-is-shown>Too few PGs per OSD warning is shown<a class=headerlink href=#too-few-pgs-per-osd-warning-is-shown title="Permanent link">&para;</a></h2> <h3 id=symptoms_8>Symptoms<a class=headerlink href=#symptoms_8 title="Permanent link">&para;</a></h3> <ul> <li><code>ceph status</code> shows "too few PGs per OSD" warning as follows.</li> </ul> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-27-1>1</a></span>
<span class=normal><a href=#__codelineno-27-2>2</a></span>
<span class=normal><a href=#__codelineno-27-3>3</a></span>
<span class=normal><a href=#__codelineno-27-4>4</a></span>
<span class=normal><a href=#__codelineno-27-5>5</a></span>
<span class=normal><a href=#__codelineno-27-6>6</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-27-1 name=__codelineno-27-1></a><span class=gp>$ </span>ceph<span class=w> </span>status
<a id=__codelineno-27-2 name=__codelineno-27-2></a><span class=go>  cluster:</span>
<a id=__codelineno-27-3 name=__codelineno-27-3></a><span class=go>    id:     fd06d7c3-5c5c-45ca-bdea-1cf26b783065</span>
<a id=__codelineno-27-4 name=__codelineno-27-4></a><span class=go>    health: HEALTH_WARN</span>
<a id=__codelineno-27-5 name=__codelineno-27-5></a><span class=go>            too few PGs per OSD (16 &lt; min 30)</span>
<a id=__codelineno-27-6 name=__codelineno-27-6></a><span class=go>[...]</span>
</code></pre></div></td></tr></table></div> <h3 id=solution_9>Solution<a class=headerlink href=#solution_9 title="Permanent link">&para;</a></h3> <p>The meaning of this warning is written in <a href=https://docs.ceph.com/docs/master/rados/operations/health-checks#too-few-pgs>the document</a>. However, in many cases it is benign. For more information, please see <a href=http://ceph.com/community/new-luminous-pg-overdose-protection/ >the blog entry</a>. Please refer to <a href=../../Storage-Configuration/Advanced/ceph-configuration/#configuring-pools>Configuring Pools</a> if you want to know the proper <code>pg_num</code> of pools and change these values.</p> <h2 id=lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc>LVM metadata can be corrupted with OSD on LV-backed PVC<a class=headerlink href=#lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc title="Permanent link">&para;</a></h2> <h3 id=symptoms_9>Symptoms<a class=headerlink href=#symptoms_9 title="Permanent link">&para;</a></h3> <p>There is a critical flaw in OSD on LV-backed PVC. LVM metadata can be corrupted if both the host and OSD container modify it simultaneously. For example, the administrator might modify it on the host, while the OSD initialization process in a container could modify it too. In addition, if <code>lvmetad</code> is running, the possibility of occurrence gets higher. In this case, the change of LVM metadata in OSD container is not reflected to LVM metadata cache in host for a while.</p> <p>If you still decide to configure an OSD on LVM, please keep the following in mind to reduce the probability of this issue.</p> <h3 id=solution_10>Solution<a class=headerlink href=#solution_10 title="Permanent link">&para;</a></h3> <ul> <li>Disable <code>lvmetad.</code></li> <li>Avoid configuration of LVs from the host. In addition, don't touch the VGs and physical volumes that back these LVs.</li> <li>Avoid incrementing the <code>count</code> field of <code>storageClassDeviceSets</code> and create a new LV that backs an OSD simultaneously.</li> </ul> <p>You can know whether the above-mentioned tag exists with the command: <code>sudo lvs -o lv_name,lv_tags</code>. If the <code>lv_tag</code> field is empty in an LV corresponding to the OSD lv_tags, this OSD encountered the problem. In this case, please <a href=../../Storage-Configuration/Advanced/ceph-osd-mgmt/#remove-an-osd>retire this OSD</a> or replace with other new OSD before restarting.</p> <p>This problem doesn't happen in newly created LV-backed PVCs because OSD container doesn't modify LVM metadata anymore. The existing lvm mode OSDs work continuously even thought upgrade your Rook. However, using the raw mode OSDs is recommended because of the above-mentioned problem. You can replace the existing OSDs with raw mode OSDs by retiring them and adding new OSDs one by one. See the documents <a href=../../Storage-Configuration/Advanced/ceph-osd-mgmt/#remove-an-osd>Remove an OSD</a> and <a href=../../Storage-Configuration/Advanced/ceph-osd-mgmt/#add-an-osd-on-a-pvc>Add an OSD on a PVC</a>.</p> <h2 id=osd-prepare-job-fails-due-to-low-aio-max-nr-setting>OSD prepare job fails due to low aio-max-nr setting<a class=headerlink href=#osd-prepare-job-fails-due-to-low-aio-max-nr-setting title="Permanent link">&para;</a></h2> <p>If the Kernel is configured with a low <a href=https://www.kernel.org/doc/Documentation/sysctl/fs.txt>aio-max-nr setting</a>, the OSD prepare job might fail with the following error:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-28-1>1</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-28-1 name=__codelineno-28-1></a>exec: stderr: 2020-09-17T00:30:12.145+0000 7f0c17632f40 -1 bdev(0x56212de88700 /var/lib/ceph/osd/ceph-0//block) _aio_start io_setup(2) failed with EAGAIN; try increasing /proc/sys/fs/aio-max-nr
</code></pre></div></td></tr></table></div> <p>To overcome this, you need to increase the value of <code>fs.aio-max-nr</code> of your sysctl configuration (typically <code>/etc/sysctl.conf</code>). You can do this with your favorite configuration management system.</p> <p>Alternatively, you can have a <a href=https://github.com/rook/rook/issues/6279#issuecomment-694390514>DaemonSet</a> to apply the configuration for you on all your nodes.</p> <h2 id=unexpected-partitions-created>Unexpected partitions created<a class=headerlink href=#unexpected-partitions-created title="Permanent link">&para;</a></h2> <h3 id=symptoms_10>Symptoms<a class=headerlink href=#symptoms_10 title="Permanent link">&para;</a></h3> <p><strong>Users running Rook versions v1.6.0-v1.6.7 may observe unwanted OSDs on partitions that appear unexpectedly and seemingly randomly, which can corrupt existing OSDs.</strong></p> <p>Unexpected partitions are created on host disks that are used by Ceph OSDs. This happens more often on SSDs than HDDs and usually only on disks that are 875GB or larger. Many tools like <code>lsblk</code>, <code>blkid</code>, <code>udevadm</code>, and <code>parted</code> will not show a partition table type for the partition. Newer versions of <code>blkid</code> are generally able to recognize the type as "atari".</p> <p>The underlying issue causing this is Atari partition (sometimes identified as AHDI) support in the Linux kernel. Atari partitions have very relaxed specifications compared to other partition types, and it is relatively easy for random data written to a disk to appear as an Atari partition to the Linux kernel. Ceph's Bluestore OSDs have an anecdotally high probability of writing data on to disks that can appear to the kernel as an Atari partition.</p> <p>Below is an example of <code>lsblk</code> output from a node where phantom Atari partitions are present. Note that <code>sdX1</code> is never present for the phantom partitions, and <code>sdX2</code> is 48G on all disks. <code>sdX3</code> is a variable size and may not always be present. It is possible for <code>sdX4</code> to appear, though it is an anecdotally rare event.</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-29-1> 1</a></span>
<span class=normal><a href=#__codelineno-29-2> 2</a></span>
<span class=normal><a href=#__codelineno-29-3> 3</a></span>
<span class=normal><a href=#__codelineno-29-4> 4</a></span>
<span class=normal><a href=#__codelineno-29-5> 5</a></span>
<span class=normal><a href=#__codelineno-29-6> 6</a></span>
<span class=normal><a href=#__codelineno-29-7> 7</a></span>
<span class=normal><a href=#__codelineno-29-8> 8</a></span>
<span class=normal><a href=#__codelineno-29-9> 9</a></span>
<span class=normal><a href=#__codelineno-29-10>10</a></span>
<span class=normal><a href=#__codelineno-29-11>11</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-29-1 name=__codelineno-29-1></a><span class=gp># </span>lsblk
<a id=__codelineno-29-2 name=__codelineno-29-2></a><span class=go>NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</span>
<a id=__codelineno-29-3 name=__codelineno-29-3></a><span class=go>sdb      8:16   0     3T  0 disk</span>
<a id=__codelineno-29-4 name=__codelineno-29-4></a><span class=go>├─sdb2   8:18   0    48G  0 part</span>
<a id=__codelineno-29-5 name=__codelineno-29-5></a><span class=go>└─sdb3   8:19   0   6.1M  0 part</span>
<a id=__codelineno-29-6 name=__codelineno-29-6></a><span class=go>sdc      8:32   0     3T  0 disk</span>
<a id=__codelineno-29-7 name=__codelineno-29-7></a><span class=go>├─sdc2   8:34   0    48G  0 part</span>
<a id=__codelineno-29-8 name=__codelineno-29-8></a><span class=go>└─sdc3   8:35   0   6.2M  0 part</span>
<a id=__codelineno-29-9 name=__codelineno-29-9></a><span class=go>sdd      8:48   0     3T  0 disk</span>
<a id=__codelineno-29-10 name=__codelineno-29-10></a><span class=go>├─sdd2   8:50   0    48G  0 part</span>
<a id=__codelineno-29-11 name=__codelineno-29-11></a><span class=go>└─sdd3   8:51   0   6.3M  0 part</span>
</code></pre></div></td></tr></table></div> <p>You can see <a href=https://github.com/rook/rook/issues/7940>GitHub rook/rook - Issue 7940 unexpected partition on disks &gt;= 1TB (atari partitions)</a> for more detailed information and discussion.</p> <h3 id=solution_11>Solution<a class=headerlink href=#solution_11 title="Permanent link">&para;</a></h3> <h4 id=recover-from-corruption-v160-v167>Recover from corruption (v1.6.0-v1.6.7)<a class=headerlink href=#recover-from-corruption-v160-v167 title="Permanent link">&para;</a></h4> <p>If you are using Rook v1.6, you must first update to v1.6.8 or higher to avoid further incidents of OSD corruption caused by these Atari partitions.</p> <p>An old workaround suggested using <code>deviceFilter: ^sd[a-z]+$</code>, but this still results in unexpected partitions. Rook will merely stop creating new OSDs on the partitions. It does not fix a related issue that <code>ceph-volume</code> that is unaware of the Atari partition problem. Users who used this workaround are still at risk for OSD failures in the future.</p> <p>To resolve the issue, immediately update to v1.6.8 or higher. After the update, no corruption should occur on OSDs created in the future. Next, to get back to a healthy Ceph cluster state, focus on one corrupted disk at a time and <a href=../../Storage-Configuration/Advanced/ceph-osd-mgmt/#remove-an-osd>remove all OSDs on each corrupted disk</a> one disk at a time.</p> <p>As an example, you may have <code>/dev/sdb</code> with two unexpected partitions (<code>/dev/sdb2</code> and <code>/dev/sdb3</code>) as well as a second corrupted disk <code>/dev/sde</code> with one unexpected partition (<code>/dev/sde2</code>).</p> <ol> <li>First, remove the OSDs associated with <code>/dev/sdb</code>, <code>/dev/sdb2</code>, and <code>/dev/sdb3</code>. There might be only one, or up to 3 OSDs depending on how your system was affected. Again see the <a href=../../Storage-Configuration/Advanced/ceph-osd-mgmt/#remove-an-osd>OSD management doc</a>.</li> <li>Use <code>dd</code> to wipe the first sectors of the partitions followed by the disk itself. E.g.,<ul> <li><code>dd if=/dev/zero of=/dev/sdb2 bs=1M</code></li> <li><code>dd if=/dev/zero of=/dev/sdb3 bs=1M</code></li> <li><code>dd if=/dev/zero of=/dev/sdb bs=1M</code></li> </ul> </li> <li>Then wipe clean <code>/dev/sdb</code> to prepare it for a new OSD. See <a href=../../Storage-Configuration/ceph-teardown/#zapping-devices>the teardown document</a> for details.</li> <li>After this, scale up the Rook operator to deploy a new OSD to <code>/dev/sdb</code>. This will allow Ceph to use <code>/dev/sdb</code> for data recovery and replication while the next OSDs are removed.</li> <li>Now Repeat steps 1-4 for <code>/dev/sde</code> and <code>/dev/sde2</code>, and continue for any other corrupted disks.</li> </ol> <p>If your Rook cluster does not have any critical data stored in it, it may be simpler to uninstall Rook completely and redeploy with v1.6.8 or higher.</p> <h2 id=operator-environment-variables-are-ignored>Operator environment variables are ignored<a class=headerlink href=#operator-environment-variables-are-ignored title="Permanent link">&para;</a></h2> <h3 id=symptoms_11>Symptoms<a class=headerlink href=#symptoms_11 title="Permanent link">&para;</a></h3> <p>Configuration settings passed as environment variables do not take effect as expected. For example, the discover daemonset is not created, even though <code>ROOK_ENABLE_DISCOVERY_DAEMON="true"</code> is set.</p> <h3 id=investigation_7>Investigation<a class=headerlink href=#investigation_7 title="Permanent link">&para;</a></h3> <p>Inspect the <code>rook-ceph-operator-config</code> ConfigMap for conflicting settings. The ConfigMap takes precedence over the environment. The ConfigMap <a href=../../Storage-Configuration/Advanced/ceph-configuration/#configuration-using-environment-variables>must exist</a>, even if all actual configuration is supplied through the environment.</p> <p>Look for lines with the <code>op-k8sutil</code> prefix in the operator logs. These lines detail the final values, and source, of the different configuration variables.</p> <p>Verify that both of the following messages are present in the operator logs:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-30-1>1</a></span>
<span class=normal><a href=#__codelineno-30-2>2</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-30-1 name=__codelineno-30-1></a>rook-ceph-operator-config-controller successfully started
<a id=__codelineno-30-2 name=__codelineno-30-2></a>rook-ceph-operator-config-controller done reconciling
</code></pre></div></td></tr></table></div> <h3 id=solution_12>Solution<a class=headerlink href=#solution_12 title="Permanent link">&para;</a></h3> <p>If it does not exist, create an empty ConfigMap:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-31-1>1</a></span>
<span class=normal><a href=#__codelineno-31-2>2</a></span>
<span class=normal><a href=#__codelineno-31-3>3</a></span>
<span class=normal><a href=#__codelineno-31-4>4</a></span>
<span class=normal><a href=#__codelineno-31-5>5</a></span>
<span class=normal><a href=#__codelineno-31-6>6</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-31-1 name=__codelineno-31-1></a><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span>
<a id=__codelineno-31-2 name=__codelineno-31-2></a><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id=__codelineno-31-3 name=__codelineno-31-3></a><span class=nt>metadata</span><span class=p>:</span>
<a id=__codelineno-31-4 name=__codelineno-31-4></a><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-ceph-operator-config</span>
<a id=__codelineno-31-5 name=__codelineno-31-5></a><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">rook-ceph</span><span class=w> </span><span class=c1># namespace:operator</span>
<a id=__codelineno-31-6 name=__codelineno-31-6></a><span class=nt>data</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{}</span>
</code></pre></div></td></tr></table></div> <p>If the ConfigMap exists, remove any keys that you wish to configure through the environment.</p> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> <a href=/ class=logo> <img src=https://rook.io/images/rook-logo-small.svg alt="rook.io logo"> </a> <p> &#169; Rook Authors 2022. Documentation distributed under <a href=https://creativecommons.org/licenses/by/4.0>CC-BY-4.0</a>. </p> <p> &#169; 2022 The Linux Foundation. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage/ >Trademark Usage</a> page. </p> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://slack.rook.io/ target=_blank rel=noopener title=slack.rook.io class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg> </a> <a href=https://twitter.com/rook_io target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://groups.google.com/forum/#!forum/rook-dev target=_blank rel=noopener title=groups.google.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M128 0c-17.7 0-32 14.3-32 32v192h96v-32c0-35.3 28.7-64 64-64h224V32c0-17.7-14.3-32-32-32H128zm128 160c-17.7 0-32 14.3-32 32v32h96c35.3 0 64 28.7 64 64v128h192c17.7 0 32-14.3 32-32V192c0-17.7-14.3-32-32-32H256zm240 64h32c8.8 0 16 7.2 16 16v32c0 8.8-7.2 16-16 16h-32c-8.8 0-16-7.2-16-16v-32c0-8.8 7.2-16 16-16zM64 256c-17.7 0-32 14.3-32 32v13l155.1 114.9c1.4 1 3.1 1.6 4.9 1.6s3.5-.6 4.9-1.6L352 301v-13c0-17.7-14.3-32-32-32H64zm288 84.8L216 441.6c-6.9 5.1-15.3 7.9-24 7.9s-17-2.8-24-7.9L32 340.8V480c0 17.7 14.3 32 32 32h256c17.7 0 32-14.3 32-32V340.8z"/></svg> </a> <a href=https://blog.rook.io/ target=_blank rel=noopener title=blog.rook.io class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262Zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.tabs.link", "instant", "navigation.expand", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "tabs"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest-release", "provider": "mike"}}</script> <script src=../../assets/javascripts/bundle.fe8b6f2b.min.js></script> </body> </html>